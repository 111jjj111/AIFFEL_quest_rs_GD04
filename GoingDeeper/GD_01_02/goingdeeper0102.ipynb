{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e50549eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fe12435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ea9b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n",
      "문장의 최단 길이: 10\n",
      "문장의 최장 길이: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "max_len = 150\n",
    "min_len = 10\n",
    "    \n",
    "print(\"Data Size:\", len(raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)\n",
    "\n",
    "cleaned_corpus = list(set(raw)) \n",
    "\n",
    "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b69304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=16000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=14.4669 num_tokens=682473 num_tokens/piece=38.7769\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=14.4447 num_tokens=682479 num_tokens/piece=38.7772\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 535805 May  9 06:58 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 312443 May  9 06:58 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece 모델 학습\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 16000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={} --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d47475fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1074, 12, 691, 10, 3212, 12, 304, 41, 4]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cab9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 함수 작성\n",
    "from tqdm import tqdm\n",
    "def sp_tokenize(s, corpus, maxlen=150,add_bos=True, add_eos=True): \n",
    "\n",
    "    tensor = []\n",
    "    bos_id = s.bos_id()\n",
    "    eos_id = s.eos_id()\n",
    "    \n",
    "    corpus = corpus.astype(str)\n",
    "\n",
    "    for sen in tqdm(corpus, desc=\"SentencePiece Tokenizing\"):\n",
    "        ids = s.EncodeAsIds(sen)\n",
    "        if add_bos:\n",
    "            ids = [bos_id] + ids\n",
    "        if add_eos:\n",
    "            ids = ids + [eos_id]\n",
    "        tensor.append(ids)\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=maxlen)\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f9520b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리     0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나     1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다     0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정     0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...     1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    file = f.readlines()\n",
    "    column_name = file[0].strip().split('\\t')\n",
    "    data_split = [x.strip().split('\\t')for x in file[1:]]\n",
    "    data = pd.DataFrame(data_split, columns=column_name)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad0152de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(seq):\n",
    "    seq = str(seq).lower()\n",
    "    \n",
    "    slang_map = {\n",
    "        \"ㅅㅂ\": \"시발\",\n",
    "        \"ㅄ\": \"병신\",\n",
    "        \"ㅈㄴ\": \"아주\",\n",
    "        \"ㅂㅅ\": \"병신\",\n",
    "        \"ㅁㅊ\": \"미친\",\n",
    "    }\n",
    "\n",
    "    # 패턴을 하나로 결합\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, slang_map.keys())) + r')\\b'\n",
    "\n",
    "    # 매치된 문자열을 dict에서 찾아서 치환\n",
    "    seq = re.sub(pattern, lambda m: slang_map[m.group()], seq)\n",
    "    \n",
    "    # 'O'만 구성된 단어 중 반복된 것만 욕설로 치환\n",
    "    seq = re.sub(r'\\bO{2,}\\b', '욕설', seq)\n",
    "    \n",
    "    return seq\n",
    "data['document'] = data['document'].astype(str).apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9792b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenizing: 100%|██████████| 150000/150000 [00:03<00:00, 41691.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tensor, word_index, index_word = sp_tokenize(s, data.iloc[:]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "866d9780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '.': 4,\n",
       " '을': 5,\n",
       " '▁': 6,\n",
       " '의': 7,\n",
       " '를': 8,\n",
       " '는': 9,\n",
       " '에': 10,\n",
       " '이': 11,\n",
       " '가': 12,\n",
       " '은': 13,\n",
       " ',': 14,\n",
       " '고': 15,\n",
       " '에서': 16,\n",
       " '▁“': 17,\n",
       " '로': 18,\n",
       " '”': 19,\n",
       " '한': 20,\n",
       " '인': 21,\n",
       " '일': 22,\n",
       " ')': 23,\n",
       " '(': 24,\n",
       " '▁이': 25,\n",
       " '과': 26,\n",
       " '▁있다': 27,\n",
       " '으로': 28,\n",
       " '와': 29,\n",
       " '▁수': 30,\n",
       " '도': 31,\n",
       " '▁밝혔다': 32,\n",
       " '▁말했다': 33,\n",
       " '할': 34,\n",
       " '년': 35,\n",
       " '지': 36,\n",
       " '▁있는': 37,\n",
       " '며': 38,\n",
       " '▁그': 39,\n",
       " '하고': 40,\n",
       " '다': 41,\n",
       " '하는': 42,\n",
       " '했다': 43,\n",
       " '▁그는': 44,\n",
       " '▁전': 45,\n",
       " '▁2': 46,\n",
       " '▁1': 47,\n",
       " '▁대한': 48,\n",
       " '▁위해': 49,\n",
       " '만': 50,\n",
       " '월': 51,\n",
       " '▁전했다': 52,\n",
       " '▁한': 53,\n",
       " '▁미국': 54,\n",
       " '해': 55,\n",
       " '▁이번': 56,\n",
       " '▁3': 57,\n",
       " '기': 58,\n",
       " '▁지난': 59,\n",
       " '현지시간': 60,\n",
       " '▁중': 61,\n",
       " '▁대해': 62,\n",
       " '자': 63,\n",
       " '\"': 64,\n",
       " '된': 65,\n",
       " '▁미': 66,\n",
       " '▁것으로': 67,\n",
       " '▁‘': 68,\n",
       " '에게': 69,\n",
       " '스': 70,\n",
       " '▁것이라고': 71,\n",
       " '명이': 72,\n",
       " '▁\"': 73,\n",
       " '▁것': 74,\n",
       " '이라고': 75,\n",
       " '▁있다고': 76,\n",
       " '▁것을': 77,\n",
       " 's': 78,\n",
       " '▁4': 79,\n",
       " '나': 80,\n",
       " '’': 81,\n",
       " '▁6': 82,\n",
       " '▁이라크': 83,\n",
       " '시': 84,\n",
       " '▁그러나': 85,\n",
       " '리': 86,\n",
       " '▁5': 87,\n",
       " '게': 88,\n",
       " '▁더': 89,\n",
       " '▁다른': 90,\n",
       " '히': 91,\n",
       " '들이': 92,\n",
       " '▁대통령': 93,\n",
       " '하게': 94,\n",
       " '▁중국': 95,\n",
       " '운': 96,\n",
       " '▁한편': 97,\n",
       " '하기': 98,\n",
       " '라': 99,\n",
       " '▁10': 100,\n",
       " '라고': 101,\n",
       " '▁북한': 102,\n",
       " '하지': 103,\n",
       " '던': 104,\n",
       " '명의': 105,\n",
       " '▁후': 106,\n",
       " '▁주': 107,\n",
       " '적인': 108,\n",
       " '대': 109,\n",
       " '▁이후': 110,\n",
       " '주': 111,\n",
       " '어': 112,\n",
       " '▁통해': 113,\n",
       " '사': 114,\n",
       " '-': 115,\n",
       " '▁정부': 116,\n",
       " '▁많은': 117,\n",
       " '들은': 118,\n",
       " '▁또': 119,\n",
       " '▁위한': 120,\n",
       " '▁대통령은': 121,\n",
       " '서': 122,\n",
       " '▁두': 123,\n",
       " '▁부시': 124,\n",
       " '▁영국': 125,\n",
       " '명': 126,\n",
       " '▁8': 127,\n",
       " '▁가장': 128,\n",
       " '▁동안': 129,\n",
       " '드': 130,\n",
       " '▁내': 131,\n",
       " '▁같은': 132,\n",
       " '▁것이다': 133,\n",
       " '▁7': 134,\n",
       " '▁대변인은': 135,\n",
       " '▁바': 136,\n",
       " '세': 137,\n",
       " '▁한국': 138,\n",
       " '트': 139,\n",
       " '▁등': 140,\n",
       " '까지': 141,\n",
       " '될': 142,\n",
       " '▁주장했다': 143,\n",
       " '이다': 144,\n",
       " '▁알': 145,\n",
       " '수': 146,\n",
       " '▁될': 147,\n",
       " '▁약': 148,\n",
       " '▁것은': 149,\n",
       " '▁한다': 150,\n",
       " '▁세계': 151,\n",
       " '▁할': 152,\n",
       " '▁또한': 153,\n",
       " '▁때': 154,\n",
       " '아': 155,\n",
       " '개': 156,\n",
       " '▁현재': 157,\n",
       " '비': 158,\n",
       " '됐다': 159,\n",
       " '했다고': 160,\n",
       " '적': 161,\n",
       " '르': 162,\n",
       " '▁12': 163,\n",
       " '▁경우': 164,\n",
       " '▁보도했다': 165,\n",
       " '▁이상': 166,\n",
       " '▁덧붙였다': 167,\n",
       " '▁관련': 168,\n",
       " '▁있었다': 169,\n",
       " '▁않았다': 170,\n",
       " '및': 171,\n",
       " '▁9': 172,\n",
       " '성': 173,\n",
       " '▁문제': 174,\n",
       " '▁최근': 175,\n",
       " '▁오바마': 176,\n",
       " '▁했다': 177,\n",
       " '했으며': 178,\n",
       " '하며': 179,\n",
       " '▁영화': 180,\n",
       " '야': 181,\n",
       " '▁일': 182,\n",
       " 'e': 183,\n",
       " '니': 184,\n",
       " '했던': 185,\n",
       " '에는': 186,\n",
       " '면': 187,\n",
       " '들을': 188,\n",
       " '▁자신의': 189,\n",
       " '부': 190,\n",
       " '▁뒤': 191,\n",
       " \"'\": 192,\n",
       " '▁함께': 193,\n",
       " '▁것이': 194,\n",
       " '▁11': 195,\n",
       " '▁일부': 196,\n",
       " '▁새로': 197,\n",
       " \"▁'\": 198,\n",
       " '미': 199,\n",
       " '크': 200,\n",
       " '▁당시': 201,\n",
       " '▁경제': 202,\n",
       " '▁20': 203,\n",
       " '상': 204,\n",
       " '▁모든': 205,\n",
       " '▁그리고': 206,\n",
       " '▁일본': 207,\n",
       " '진': 208,\n",
       " '▁때문에': 209,\n",
       " '전': 210,\n",
       " '소': 211,\n",
       " '▁말': 212,\n",
       " '▁예정이다': 213,\n",
       " '▁수도': 214,\n",
       " '▁19': 215,\n",
       " '▁없다': 216,\n",
       " '치': 217,\n",
       " '▁조사': 218,\n",
       " '▁있습니다': 219,\n",
       " '▁설명했다': 220,\n",
       " '▁지': 221,\n",
       " '▁제': 222,\n",
       " '▁몇': 223,\n",
       " '선': 224,\n",
       " '▁있으며': 225,\n",
       " '해야': 226,\n",
       " '보': 227,\n",
       " '▁경찰은': 228,\n",
       " '원': 229,\n",
       " '▁비': 230,\n",
       " '▁모두': 231,\n",
       " '▁아': 232,\n",
       " '차': 233,\n",
       " '▁미군': 234,\n",
       " '마': 235,\n",
       " '▁정부는': 236,\n",
       " '▁파키스탄': 237,\n",
       " '에서는': 238,\n",
       " '한다': 239,\n",
       " '▁프랑스': 240,\n",
       " '▁유엔': 241,\n",
       " '분': 242,\n",
       " '보다': 243,\n",
       " '억': 244,\n",
       " '▁국가': 245,\n",
       " '▁계속': 246,\n",
       " '당': 247,\n",
       " '약': 248,\n",
       " '▁큰': 249,\n",
       " '▁공격': 250,\n",
       " '파': 251,\n",
       " '▁마': 252,\n",
       " '▁30': 253,\n",
       " '하다': 254,\n",
       " '▁시': 255,\n",
       " '▁사': 256,\n",
       " '▁하는': 257,\n",
       " '화': 258,\n",
       " '▁계획': 259,\n",
       " '▁이들': 260,\n",
       " '▁러시아': 261,\n",
       " '▁지난해': 262,\n",
       " '▁이스라엘': 263,\n",
       " '장': 264,\n",
       " '적으로': 265,\n",
       " '타': 266,\n",
       " '▁경찰': 267,\n",
       " '▁민주당': 268,\n",
       " '▁사고': 269,\n",
       " '▁(': 270,\n",
       " '명을': 271,\n",
       " '▁열린': 272,\n",
       " '▁나': 273,\n",
       " '▁오': 274,\n",
       " '▁대통령이': 275,\n",
       " '▁않을': 276,\n",
       " '▁지역': 277,\n",
       " '들': 278,\n",
       " '▁올해': 279,\n",
       " '여': 280,\n",
       " '제': 281,\n",
       " '용': 282,\n",
       " '오': 283,\n",
       " '하': 284,\n",
       " '2': 285,\n",
       " '▁이날': 286,\n",
       " '프': 287,\n",
       " '▁총리는': 288,\n",
       " '▁우리': 289,\n",
       " '그': 290,\n",
       " '▁the': 291,\n",
       " '바': 292,\n",
       " '▁반': 293,\n",
       " '▁발표했다': 294,\n",
       " '▁경기': 295,\n",
       " '▁핵': 296,\n",
       " '▁클린턴': 297,\n",
       " '▁발생한': 298,\n",
       " '즈': 299,\n",
       " '▁지원': 300,\n",
       " '정': 301,\n",
       " '▁15': 302,\n",
       " '했습니다': 303,\n",
       " '신': 304,\n",
       " '▁받고': 305,\n",
       " '티': 306,\n",
       " '▁후보': 307,\n",
       " 'd': 308,\n",
       " '▁테러': 309,\n",
       " '라는': 310,\n",
       " '조': 311,\n",
       " '간': 312,\n",
       " '에따르면': 313,\n",
       " '▁부': 314,\n",
       " '▁하고': 315,\n",
       " '▁해': 316,\n",
       " '▁결과': 317,\n",
       " '▁있을': 318,\n",
       " '▁거': 319,\n",
       " '디': 320,\n",
       " '었다': 321,\n",
       " '▁가운데': 322,\n",
       " '우': 323,\n",
       " '레': 324,\n",
       " '▁미국의': 325,\n",
       " '위': 326,\n",
       " '▁달러': 327,\n",
       " '▁안': 328,\n",
       " '▁존': 329,\n",
       " '▁16': 330,\n",
       " '시간': 331,\n",
       " '▁데': 332,\n",
       " '▁to': 333,\n",
       " '내': 334,\n",
       " '이나': 335,\n",
       " 'm': 336,\n",
       " '▁달': 337,\n",
       " '▁군': 338,\n",
       " '▁의해': 339,\n",
       " '▁여성': 340,\n",
       " '▁이란': 341,\n",
       " '▁다': 342,\n",
       " '▁된다': 343,\n",
       " '▁선거': 344,\n",
       " '▁국제': 345,\n",
       " '부터': 346,\n",
       " '▁회담': 347,\n",
       " '▁대': 348,\n",
       " '▁사건': 349,\n",
       " '으며': 350,\n",
       " '▁주요': 351,\n",
       " '▁사람들이': 352,\n",
       " '▁정부가': 353,\n",
       " '카': 354,\n",
       " '▁에': 355,\n",
       " '1': 356,\n",
       " '▁18': 357,\n",
       " '▁다시': 358,\n",
       " '▁연구': 359,\n",
       " ')’': 360,\n",
       " '면서': 361,\n",
       " '▁인해': 362,\n",
       " '▁보고': 363,\n",
       " '▁관계': 364,\n",
       " '▁됐다': 365,\n",
       " '산': 366,\n",
       " '에도': 367,\n",
       " '▁세': 368,\n",
       " '입니다': 369,\n",
       " '▁발표': 370,\n",
       " '▁않은': 371,\n",
       " '▁총리': 372,\n",
       " '길': 373,\n",
       " '▁of': 374,\n",
       " '토': 375,\n",
       " '▁초': 376,\n",
       " '질': 377,\n",
       " '러': 378,\n",
       " '▁보': 379,\n",
       " '.”': 380,\n",
       " '▁13': 381,\n",
       " '노': 382,\n",
       " '▁첫': 383,\n",
       " '▁혐의로': 384,\n",
       " '▁결정': 385,\n",
       " '▁시간': 386,\n",
       " '하면서': 387,\n",
       " '▁무': 388,\n",
       " 'S': 389,\n",
       " '▁총': 390,\n",
       " '려': 391,\n",
       " '▁있다는': 392,\n",
       " '계': 393,\n",
       " '▁채': 394,\n",
       " '▁정': 395,\n",
       " '되는': 396,\n",
       " 'y': 397,\n",
       " '코': 398,\n",
       " '모': 399,\n",
       " '린': 400,\n",
       " '▁조': 401,\n",
       " '▁노': 402,\n",
       " '▁불': 403,\n",
       " '▁조치': 404,\n",
       " '되어': 405,\n",
       " '▁이미': 406,\n",
       " '▁프로그램': 407,\n",
       " '▁시위': 408,\n",
       " '들의': 409,\n",
       " '▁14': 410,\n",
       " '▁위치한': 411,\n",
       " '▁받았다': 412,\n",
       " 'km': 413,\n",
       " '▁기': 414,\n",
       " '구': 415,\n",
       " '▁우려': 416,\n",
       " '▁강': 417,\n",
       " '▁팔레스타인': 418,\n",
       " '국': 419,\n",
       " '호': 420,\n",
       " '▁아프간': 421,\n",
       " '▁줄': 422,\n",
       " '번': 423,\n",
       " '브': 424,\n",
       " '▁인도': 425,\n",
       " 'a': 426,\n",
       " '▁없는': 427,\n",
       " '식': 428,\n",
       " '지만': 429,\n",
       " '▁방문': 430,\n",
       " '돼': 431,\n",
       " '▁성': 432,\n",
       " '▁승리': 433,\n",
       " '권': 434,\n",
       " '하여': 435,\n",
       " '▁차': 436,\n",
       " '▁아직': 437,\n",
       " '▁TV': 438,\n",
       " '▁유': 439,\n",
       " '▁감독': 440,\n",
       " '▁피해': 441,\n",
       " '▁파': 442,\n",
       " '▁모': 443,\n",
       " '이라며': 444,\n",
       " '▁추가': 445,\n",
       " '▁사실을': 446,\n",
       " '로부터': 447,\n",
       " '▁25': 448,\n",
       " '▁하': 449,\n",
       " '단': 450,\n",
       " '▁혐의': 451,\n",
       " '▁관계자는': 452,\n",
       " '▁뉴욕': 453,\n",
       " '▁않고': 454,\n",
       " '▁간': 455,\n",
       " '관': 456,\n",
       " '▁유럽': 457,\n",
       " '루': 458,\n",
       " '/': 459,\n",
       " '▁독일': 460,\n",
       " '▁회사': 461,\n",
       " '▁한다고': 462,\n",
       " '▁지적했다': 463,\n",
       " \"▁''\": 464,\n",
       " '더': 465,\n",
       " '▁최고': 466,\n",
       " '▁대선': 467,\n",
       " '▁지지': 468,\n",
       " '▁17': 469,\n",
       " '▁피': 470,\n",
       " '습니다': 471,\n",
       " '▁지금': 472,\n",
       " '달러': 473,\n",
       " '▁W': 474,\n",
       " '되고': 475,\n",
       " '▁잘': 476,\n",
       " '▁포함': 477,\n",
       " '▁기록': 478,\n",
       " '▁구': 479,\n",
       " '▁활동': 480,\n",
       " '▁아이': 481,\n",
       " 'A': 482,\n",
       " '▁명의': 483,\n",
       " '▁신': 484,\n",
       " '▁협상': 485,\n",
       " '년간': 486,\n",
       " '▁조지': 487,\n",
       " '▁매우': 488,\n",
       " '▁자': 489,\n",
       " '▁소속': 490,\n",
       " '▁카': 491,\n",
       " '▁원': 492,\n",
       " '▁재': 493,\n",
       " '▁내용': 494,\n",
       " '▁시장': 495,\n",
       " '▁올림픽': 496,\n",
       " '살': 497,\n",
       " '▁이슬람': 498,\n",
       " '▁당국은': 499,\n",
       " '물': 500,\n",
       " '실': 501,\n",
       " '▁관리': 502,\n",
       " '▁탈레반': 503,\n",
       " '▁대부분': 504,\n",
       " '직': 505,\n",
       " '난': 506,\n",
       " '▁매케인': 507,\n",
       " '든': 508,\n",
       " '중': 509,\n",
       " '동': 510,\n",
       " '▁in': 511,\n",
       " '▁호주': 512,\n",
       " '개월': 513,\n",
       " 'ing': 514,\n",
       " '▁없다고': 515,\n",
       " '▁차량': 516,\n",
       " '▁대학': 517,\n",
       " '▁타': 518,\n",
       " '법': 519,\n",
       " '▁이는': 520,\n",
       " '▁받은': 521,\n",
       " '였다': 522,\n",
       " '▁예정': 523,\n",
       " '▁못했다': 524,\n",
       " '체': 525,\n",
       " '처럼': 526,\n",
       " '▁않는다': 527,\n",
       " '점': 528,\n",
       " '▁상원의원': 529,\n",
       " '▁인터넷': 530,\n",
       " '안': 531,\n",
       " '회': 532,\n",
       " '▁사람': 533,\n",
       " '▁이에': 534,\n",
       " '3': 535,\n",
       " 'ed': 536,\n",
       " '▁비난': 537,\n",
       " '공': 538,\n",
       " '되지': 539,\n",
       " '?': 540,\n",
       " '▁2006': 541,\n",
       " '▁많': 542,\n",
       " '5': 543,\n",
       " '▁어떤': 544,\n",
       " '▁24': 545,\n",
       " '▁주장': 546,\n",
       " '▁오늘': 547,\n",
       " '▁부상': 548,\n",
       " 'o': 549,\n",
       " '▁시작': 550,\n",
       " '▁여러': 551,\n",
       " '▁하지': 552,\n",
       " '▁판매': 553,\n",
       " '력': 554,\n",
       " '▁새': 555,\n",
       " '▁하지만': 556,\n",
       " '▁폭탄': 557,\n",
       " '▁사망': 558,\n",
       " '▁100': 559,\n",
       " '▁있던': 560,\n",
       " '▁공': 561,\n",
       " '▁물': 562,\n",
       " '▁오후': 563,\n",
       " '▁21': 564,\n",
       " '▁석방': 565,\n",
       " '무': 566,\n",
       " '▁50': 567,\n",
       " '유': 568,\n",
       " '▁입장': 569,\n",
       " '발': 570,\n",
       " '▁생각': 571,\n",
       " '▁A': 572,\n",
       " '▁전쟁': 573,\n",
       " '▁당': 574,\n",
       " '명은': 575,\n",
       " '▁매': 576,\n",
       " '▁양': 577,\n",
       " '▁검찰': 578,\n",
       " '재': 579,\n",
       " '▁방송': 580,\n",
       " '▁상': 581,\n",
       " '종': 582,\n",
       " '경': 583,\n",
       " '▁도': 584,\n",
       " '▁날': 585,\n",
       " '터': 586,\n",
       " '▁미얀마': 587,\n",
       " '▁27': 588,\n",
       " '▁기자': 589,\n",
       " '형': 590,\n",
       " '되었다': 591,\n",
       " '릴': 592,\n",
       " '반': 593,\n",
       " '▁배': 594,\n",
       " '임': 595,\n",
       " '들에게': 596,\n",
       " '▁정책': 597,\n",
       " '피': 598,\n",
       " '▁있어': 599,\n",
       " '하는데': 600,\n",
       " '▁해결': 601,\n",
       " '이며': 602,\n",
       " '▁리': 603,\n",
       " '데': 604,\n",
       " '▁자동차': 605,\n",
       " '개의': 606,\n",
       " '▁23': 607,\n",
       " '▁된': 608,\n",
       " '▁반대': 609,\n",
       " '▁인터뷰에서': 610,\n",
       " '▁준비': 611,\n",
       " '▁소': 612,\n",
       " '이었다': 613,\n",
       " '▁병원': 614,\n",
       " '▁22': 615,\n",
       " '▁우주': 616,\n",
       " '▁라': 617,\n",
       " '▁김': 618,\n",
       " '▁컴퓨터': 619,\n",
       " '▁발': 620,\n",
       " '▁강조했다': 621,\n",
       " '▁것에': 622,\n",
       " '▁이러': 623,\n",
       " '했지만': 624,\n",
       " '▁갖고': 625,\n",
       " '했고': 626,\n",
       " '▁노력': 627,\n",
       " '▁스': 628,\n",
       " '포': 629,\n",
       " '▁그들은': 630,\n",
       " '▁개발': 631,\n",
       " '감': 632,\n",
       " '▁기술': 633,\n",
       " '▁공화당': 634,\n",
       " '▁점': 635,\n",
       " '▁도시': 636,\n",
       " 'i': 637,\n",
       " '▁합의': 638,\n",
       " '들과': 639,\n",
       " '▁S': 640,\n",
       " '▁개': 641,\n",
       " '▁앞서': 642,\n",
       " '▁산': 643,\n",
       " '▁떨어진': 644,\n",
       " '석': 645,\n",
       " '▁원문기사보기': 646,\n",
       " '▁있도록': 647,\n",
       " '▁오전': 648,\n",
       " '▁대표': 649,\n",
       " '▁서': 650,\n",
       " '▁사이': 651,\n",
       " '▁발생': 652,\n",
       " '▁마지막': 653,\n",
       " '▁남부': 654,\n",
       " 't': 655,\n",
       " '▁비난했다': 656,\n",
       " '▁투표': 657,\n",
       " '▁이어': 658,\n",
       " '▁북한이': 659,\n",
       " ':': 660,\n",
       " '▁터키': 661,\n",
       " '키': 662,\n",
       " '온': 663,\n",
       " '▁안전': 664,\n",
       " '▁가진': 665,\n",
       " '▁폭발': 666,\n",
       " '▁경찰이': 667,\n",
       " '▁증가': 668,\n",
       " '연': 669,\n",
       " '▁오는': 670,\n",
       " '▁변화': 671,\n",
       " '에따라': 672,\n",
       " '하기로': 673,\n",
       " '▁2005': 674,\n",
       " '심': 675,\n",
       " '했으나': 676,\n",
       " '▁미국과': 677,\n",
       " '▁의사': 678,\n",
       " '▁볼': 679,\n",
       " '네': 680,\n",
       " '7': 681,\n",
       " '▁장관은': 682,\n",
       " 'c': 683,\n",
       " '▁이를': 684,\n",
       " '▁인근': 685,\n",
       " '▁외': 686,\n",
       " '▁자신이': 687,\n",
       " '▁처음으로': 688,\n",
       " '▁상황': 689,\n",
       " '▁그녀': 690,\n",
       " '방': 691,\n",
       " '버': 692,\n",
       " '▁처음': 693,\n",
       " '거': 694,\n",
       " 'al': 695,\n",
       " '▁그녀는': 696,\n",
       " 'C': 697,\n",
       " '이라는': 698,\n",
       " '문': 699,\n",
       " '▁위협': 700,\n",
       " '▁규모': 701,\n",
       " '했었다': 702,\n",
       " '▁사용': 703,\n",
       " '▁생산': 704,\n",
       " '▁남자': 705,\n",
       " '▁버락': 706,\n",
       " '▁성명': 707,\n",
       " '으로부터': 708,\n",
       " '▁지난달': 709,\n",
       " '▁반군': 710,\n",
       " '군': 711,\n",
       " '▁아시아': 712,\n",
       " '▁치료': 713,\n",
       " 'P': 714,\n",
       " '▁없었다': 715,\n",
       " '▁여행': 716,\n",
       " '▁26': 717,\n",
       " '▁선': 718,\n",
       " '▁힐러리': 719,\n",
       " '▁이탈리아': 720,\n",
       " '학': 721,\n",
       " '▁40': 722,\n",
       " '▁최소': 723,\n",
       " '▁지역에서': 724,\n",
       " '▁코': 725,\n",
       " '▁금융': 726,\n",
       " '▁정치': 727,\n",
       " '거나': 728,\n",
       " '▁연방': 729,\n",
       " '▁앞으로': 730,\n",
       " '▁CNN': 731,\n",
       " '▁있지만': 732,\n",
       " '▁a': 733,\n",
       " '▁영향': 734,\n",
       " '하도록': 735,\n",
       " '▁야당': 736,\n",
       " '▁열': 737,\n",
       " '▁없이': 738,\n",
       " '6': 739,\n",
       " '▁보인다': 740,\n",
       " '▁보도': 741,\n",
       " '▁관한': 742,\n",
       " '위원회': 743,\n",
       " '▁̋': 744,\n",
       " '▁사망했다': 745,\n",
       " '지는': 746,\n",
       " '▁장': 747,\n",
       " '▁다음': 748,\n",
       " '▁불법': 749,\n",
       " '▁실': 750,\n",
       " '와의': 751,\n",
       " 'n': 752,\n",
       " '▁이용': 753,\n",
       " '▁스페인': 754,\n",
       " '교': 755,\n",
       " '요': 756,\n",
       " '▁중단': 757,\n",
       " '▁않다': 758,\n",
       " '▁사망자': 759,\n",
       " '▁만에': 760,\n",
       " '▁좋은': 761,\n",
       " '▁의회': 762,\n",
       " '▁살해': 763,\n",
       " '됐다고': 764,\n",
       " '▁미국이': 765,\n",
       " '강': 766,\n",
       " '배': 767,\n",
       " '▁29': 768,\n",
       " '▁사람들': 769,\n",
       " '▁이름': 770,\n",
       " '츠': 771,\n",
       " '스트': 772,\n",
       " '▁현지': 773,\n",
       " 'er': 774,\n",
       " '▁가격': 775,\n",
       " '번째': 776,\n",
       " '_': 777,\n",
       " 'E': 778,\n",
       " '된다': 779,\n",
       " '▁당국': 780,\n",
       " '▁발견': 781,\n",
       " '▁상태': 782,\n",
       " '▁은행': 783,\n",
       " '▁공식': 784,\n",
       " '우리는': 785,\n",
       " '란': 786,\n",
       " '▁사태': 787,\n",
       " '▁런던': 788,\n",
       " 'N': 789,\n",
       " '▁행사': 790,\n",
       " '▁언론': 791,\n",
       " '▁미사일': 792,\n",
       " '▁가능성': 793,\n",
       " '▁팀': 794,\n",
       " '는데': 795,\n",
       " '▁밤': 796,\n",
       " '▁사람들은': 797,\n",
       " '▁평화': 798,\n",
       " '▁28': 799,\n",
       " '▁거래': 800,\n",
       " '▁어': 801,\n",
       " '건': 802,\n",
       " '론': 803,\n",
       " '▁무기': 804,\n",
       " 'article': 805,\n",
       " '▁백악관': 806,\n",
       " '▁적': 807,\n",
       " '▁이들은': 808,\n",
       " '▁주둔': 809,\n",
       " '▁알려졌다': 810,\n",
       " 'M': 811,\n",
       " '날': 812,\n",
       " '▁자신': 813,\n",
       " '천': 814,\n",
       " '▁AP': 815,\n",
       " '▁논의': 816,\n",
       " '▁벌이': 817,\n",
       " '▁심': 818,\n",
       " '▁불구하고': 819,\n",
       " '▁중요한': 820,\n",
       " '▁연기': 821,\n",
       " '▁오바마는': 822,\n",
       " '됐으며': 823,\n",
       " 'u': 824,\n",
       " '▁집': 825,\n",
       " '▁발언': 826,\n",
       " '▁말했습니다': 827,\n",
       " '▁바그다드': 828,\n",
       " '▁이전': 829,\n",
       " '▁발생했다': 830,\n",
       " '▁박사는': 831,\n",
       " 'p': 832,\n",
       " '▁명': 833,\n",
       " '▁대통령과': 834,\n",
       " '▁사망한': 835,\n",
       " '▁게임': 836,\n",
       " '두': 837,\n",
       " '▁필요': 838,\n",
       " '병': 839,\n",
       " '행': 840,\n",
       " '▁우': 841,\n",
       " '▁것입니다': 842,\n",
       " '청': 843,\n",
       " '▁지구': 844,\n",
       " '▁자금': 845,\n",
       " '▁용의자': 846,\n",
       " '▁B': 847,\n",
       " '▁이런': 848,\n",
       " '급': 849,\n",
       " '▁정도': 850,\n",
       " '▁미국은': 851,\n",
       " '▁역할': 852,\n",
       " '▁반면': 853,\n",
       " '▁돈': 854,\n",
       " '▁조직': 855,\n",
       " '너': 856,\n",
       " 'I': 857,\n",
       " '▁관심': 858,\n",
       " '▁올': 859,\n",
       " '▁법원': 860,\n",
       " '▁코리아': 861,\n",
       " '▁2008': 862,\n",
       " '▁비행기': 863,\n",
       " '▁공동': 864,\n",
       " '▁투자': 865,\n",
       " '▁인질': 866,\n",
       " '▁역사': 867,\n",
       " '▁이야기': 868,\n",
       " '▁상승': 869,\n",
       " 'D': 870,\n",
       " '▁학교': 871,\n",
       " '’’': 872,\n",
       " 'and': 873,\n",
       " '▁공급': 874,\n",
       " 'T': 875,\n",
       " '▁최대': 876,\n",
       " '▁법': 877,\n",
       " '▁동': 878,\n",
       " '▁포': 879,\n",
       " '▁무샤라프': 880,\n",
       " '저': 881,\n",
       " '▁보안': 882,\n",
       " '▁북부': 883,\n",
       " '▁고위': 884,\n",
       " '▁보다': 885,\n",
       " '울': 886,\n",
       " '▁회장': 887,\n",
       " '▁사망했다고': 888,\n",
       " '장은': 889,\n",
       " '▁치': 890,\n",
       " '▁서비스': 891,\n",
       " '2005.08': 892,\n",
       " '▁방': 893,\n",
       " '▁메': 894,\n",
       " 'F': 895,\n",
       " '▁살': 896,\n",
       " '▁위험': 897,\n",
       " '마리': 898,\n",
       " '▁정치적': 899,\n",
       " '설': 900,\n",
       " '래': 901,\n",
       " '워': 902,\n",
       " '▁의하면': 903,\n",
       " '▁단체': 904,\n",
       " '▁입': 905,\n",
       " '▁알카에다': 906,\n",
       " '▁자신들': 907,\n",
       " '▁앞': 908,\n",
       " '▁모습을': 909,\n",
       " '▁2005.08': 910,\n",
       " '금': 911,\n",
       " '▁건물': 912,\n",
       " '포인트': 913,\n",
       " 'CNN': 914,\n",
       " '▁워싱턴': 915,\n",
       " '▁의원': 916,\n",
       " '▁지도자': 917,\n",
       " '▁부족': 918,\n",
       " '▁충돌': 919,\n",
       " '▁수사': 920,\n",
       " '▁발사': 921,\n",
       " '▁위': 922,\n",
       " '▁군사': 923,\n",
       " '시킬': 924,\n",
       " '▁것이라는': 925,\n",
       " '▁건강': 926,\n",
       " '▁자살': 927,\n",
       " '▁요구': 928,\n",
       " '친': 929,\n",
       " '▁행위': 930,\n",
       " '하면': 931,\n",
       " '▁공항': 932,\n",
       " '▁우승': 933,\n",
       " '▁출신': 934,\n",
       " '▁감소': 935,\n",
       " '▁여': 936,\n",
       " '▁가능성이': 937,\n",
       " '▁높은': 938,\n",
       " '▁더욱': 939,\n",
       " '▁지난주': 940,\n",
       " '▁왔다': 941,\n",
       " '기를': 942,\n",
       " '▁뜻': 943,\n",
       " '류': 944,\n",
       " '4': 945,\n",
       " '▁국경': 946,\n",
       " '▁인용': 947,\n",
       " '▁행동': 948,\n",
       " '팀': 949,\n",
       " '▁전에': 950,\n",
       " '후': 951,\n",
       " 'r': 952,\n",
       " '’(': 953,\n",
       " '▁막': 954,\n",
       " '슨': 955,\n",
       " '▁성명에서': 956,\n",
       " '▁뉴스': 957,\n",
       " '▁서울': 958,\n",
       " '작': 959,\n",
       " '▁하마스': 960,\n",
       " '하자': 961,\n",
       " '▁&': 962,\n",
       " '▁지역에': 963,\n",
       " '세의': 964,\n",
       " '간의': 965,\n",
       " '▁말을': 966,\n",
       " '▁보호': 967,\n",
       " '만달러': 968,\n",
       " '▁N': 969,\n",
       " '판': 970,\n",
       " '▁힘': 971,\n",
       " '▁하나': 972,\n",
       " '▁가족': 973,\n",
       " '▁이와': 974,\n",
       " '한다고': 975,\n",
       " '속': 976,\n",
       " '▁아니다': 977,\n",
       " 'B': 978,\n",
       " '▁것이며': 979,\n",
       " '▁인쇄': 980,\n",
       " '▁레': 981,\n",
       " '▁검사': 982,\n",
       " '▁알고': 983,\n",
       " '▁2004': 984,\n",
       " '이번': 985,\n",
       " '▁총리가': 986,\n",
       " '초': 987,\n",
       " '▁for': 988,\n",
       " '▁호': 989,\n",
       " '▁않는': 990,\n",
       " '▁분': 991,\n",
       " '▁길': 992,\n",
       " '▁독립': 993,\n",
       " '▁부토': 994,\n",
       " 'ar': 995,\n",
       " '▁전화': 996,\n",
       " '민': 997,\n",
       " '였던': 998,\n",
       " '양': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "641fc812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁아', '▁더', '빙', '.', '.', '▁진짜', '▁짜', '증', '나', '네', '요', '▁목소리', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#print(word_index)\n",
    "#print(index_word)\n",
    "\n",
    "for sequence in tensor:\n",
    "    print([index_word[word]for word in sequence])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917eca2",
   "metadata": {},
   "source": [
    "LSTM 기반의 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6078cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:03<00:00, 41934.70it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:01<00:00, 45488.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Mecab : 한국어 형태소 분석기\n",
    "#tokenizer = Mecab()\n",
    "\n",
    "def load_data(train_data, test_data, s,num_words=10000):\n",
    "    # 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document'])\n",
    "    test_data = test_data.drop_duplicates(subset=['document'])\n",
    "\n",
    "    # Nan 결측치 제거\n",
    "    # https://wikidocs.net/153202\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    # Use SentencePiece\n",
    "    train_tokens, word_to_index, _ = sp_tokenize(s, train_data['document'], maxlen=40)\n",
    "    test_tokens, _, _ = sp_tokenize(s, test_data['document'],maxlen=40)\n",
    "    \n",
    "    return train_tokens, np.array(train_data['label']), test_tokens, test_data['label'], word_to_index\n",
    "\n",
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt')\n",
    "\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5394cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126162, 40)\n",
      "(126162,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "084a627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126162, 40)\n",
      "(126162,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2661c49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "198a49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 4s 7ms/step - loss: 0.5707 - accuracy: 0.6792 - val_loss: 0.4178 - val_accuracy: 0.8178\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3916 - accuracy: 0.8302 - val_loss: 0.3908 - val_accuracy: 0.8254\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3677 - accuracy: 0.8410 - val_loss: 0.3857 - val_accuracy: 0.8259\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3572 - accuracy: 0.8463 - val_loss: 0.3926 - val_accuracy: 0.8221\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3500 - accuracy: 0.8500 - val_loss: 0.3883 - val_accuracy: 0.8270\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22e04303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/1536 - 3s - loss: 0.3940 - accuracy: 0.8245\n",
      "[0.3939913809299469, 0.824516773223877]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d379f",
   "metadata": {},
   "source": [
    "# 영화 데이터로 토크나이저 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "467e4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(data_path)\n",
    "train_data = train_data.dropna(subset=['document'])\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "clean_path = \"spm_input.txt\"\n",
    "\n",
    "with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in train_data['document']:\n",
    "        f.write(str(line).strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88d0ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=spm_input.txt --model_prefix=korean_spm --vocab_size=16000  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_input.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: spm_input.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 150000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430599\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1686\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 150000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308597 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 150000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357031\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 357031 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157511 obj=15.4277 num_tokens=841362 num_tokens/piece=5.34161\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145725 obj=14.3586 num_tokens=846836 num_tokens/piece=5.81119\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109233 obj=14.4548 num_tokens=882029 num_tokens/piece=8.07475\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109051 obj=14.3996 num_tokens=882763 num_tokens/piece=8.09496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81784 obj=14.6341 num_tokens=926725 num_tokens/piece=11.3314\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81776 obj=14.5731 num_tokens=926844 num_tokens/piece=11.3339\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61332 obj=14.8307 num_tokens=968598 num_tokens/piece=15.7927\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61332 obj=14.7702 num_tokens=968692 num_tokens/piece=15.7942\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45999 obj=15.0586 num_tokens=1013498 num_tokens/piece=22.033\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45999 obj=14.9947 num_tokens=1013492 num_tokens/piece=22.0329\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34499 obj=15.3164 num_tokens=1059614 num_tokens/piece=30.7143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34499 obj=15.2502 num_tokens=1059627 num_tokens/piece=30.7147\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25874 obj=15.6061 num_tokens=1107398 num_tokens/piece=42.7996\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25874 obj=15.534 num_tokens=1107416 num_tokens/piece=42.8003\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19405 obj=15.9185 num_tokens=1157734 num_tokens/piece=59.6616\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19405 obj=15.8384 num_tokens=1157755 num_tokens/piece=59.6627\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=15.9723 num_tokens=1175362 num_tokens/piece=66.7819\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=15.9443 num_tokens=1175368 num_tokens/piece=66.7823\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(clean_path, vocab_size)    \n",
    ")\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.load(\"korean_spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1a036fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenizing: 100%|██████████| 149995/149995 [00:05<00:00, 29064.67it/s]\n"
     ]
    }
   ],
   "source": [
    "tensor, word_index, index_word = sp_tokenize(s, train_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b36078d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '.': 4,\n",
       " '▁': 5,\n",
       " '..': 6,\n",
       " '▁영화': 7,\n",
       " '...': 8,\n",
       " '이': 9,\n",
       " '의': 10,\n",
       " ',': 11,\n",
       " '가': 12,\n",
       " '도': 13,\n",
       " '는': 14,\n",
       " '을': 15,\n",
       " '에': 16,\n",
       " '?': 17,\n",
       " '은': 18,\n",
       " '고': 19,\n",
       " '다': 20,\n",
       " '▁너무': 21,\n",
       " '!': 22,\n",
       " '지': 23,\n",
       " '▁정말': 24,\n",
       " '를': 25,\n",
       " '▁이': 26,\n",
       " '한': 27,\n",
       " '만': 28,\n",
       " '▁진짜': 29,\n",
       " '로': 30,\n",
       " '~': 31,\n",
       " '나': 32,\n",
       " '게': 33,\n",
       " '영화': 34,\n",
       " '과': 35,\n",
       " '점': 36,\n",
       " '으로': 37,\n",
       " '▁잘': 38,\n",
       " '▁왜': 39,\n",
       " '하고': 40,\n",
       " '▁그': 41,\n",
       " '▁1': 42,\n",
       " '!!': 43,\n",
       " '▁더': 44,\n",
       " '인': 45,\n",
       " '▁이런': 46,\n",
       " '▁다': 47,\n",
       " '▁수': 48,\n",
       " '에서': 49,\n",
       " '라': 50,\n",
       " '기': 51,\n",
       " '와': 52,\n",
       " '....': 53,\n",
       " '서': 54,\n",
       " '▁그냥': 55,\n",
       " '▁안': 56,\n",
       " '▁아': 57,\n",
       " '어': 58,\n",
       " '네': 59,\n",
       " '아': 60,\n",
       " '하는': 61,\n",
       " '요': 62,\n",
       " '▁10': 63,\n",
       " '▁보고': 64,\n",
       " '들': 65,\n",
       " '▁본': 66,\n",
       " '리': 67,\n",
       " '해': 68,\n",
       " '▁드라마': 69,\n",
       " '▁평점': 70,\n",
       " '▁영화를': 71,\n",
       " '▁좀': 72,\n",
       " '거': 73,\n",
       " '▁연기': 74,\n",
       " '자': 75,\n",
       " '▁내': 76,\n",
       " '면': 77,\n",
       " '네요': 78,\n",
       " '▁봤는데': 79,\n",
       " '▁한': 80,\n",
       " '▁ᄏᄏ': 81,\n",
       " '▁참': 82,\n",
       " '▁영화가': 83,\n",
       " '▁영화는': 84,\n",
       " '지만': 85,\n",
       " '▁이거': 86,\n",
       " '▁없는': 87,\n",
       " '야': 88,\n",
       " '▁최고의': 89,\n",
       " '▁내가': 90,\n",
       " '수': 91,\n",
       " '▁완전': 92,\n",
       " '이다': 93,\n",
       " '▁쓰레기': 94,\n",
       " '니': 95,\n",
       " '▁없다': 96,\n",
       " '▁스토리': 97,\n",
       " '▁이건': 98,\n",
       " '▁좋은': 99,\n",
       " '▁있는': 100,\n",
       " '!!!': 101,\n",
       " '▁최고': 102,\n",
       " '스': 103,\n",
       " 'ᄏᄏ': 104,\n",
       " '대': 105,\n",
       " '▁2': 106,\n",
       " '일': 107,\n",
       " '▁이렇게': 108,\n",
       " '음': 109,\n",
       " '▁3': 110,\n",
       " '▁이게': 111,\n",
       " '▁보는': 112,\n",
       " '하게': 113,\n",
       " '▁다시': 114,\n",
       " '보다': 115,\n",
       " '해서': 116,\n",
       " '진': 117,\n",
       " '사': 118,\n",
       " '▁못': 119,\n",
       " '까지': 120,\n",
       " '함': 121,\n",
       " '임': 122,\n",
       " '▁것': 123,\n",
       " '시': 124,\n",
       " '구': 125,\n",
       " '^^': 126,\n",
       " '들이': 127,\n",
       " '▁난': 128,\n",
       " 'ᄏ': 129,\n",
       " '▁평점이': 130,\n",
       " '할': 131,\n",
       " '▁많이': 132,\n",
       " '하다': 133,\n",
       " '여': 134,\n",
       " '건': 135,\n",
       " '데': 136,\n",
       " '듯': 137,\n",
       " '랑': 138,\n",
       " '~~': 139,\n",
       " '냐': 140,\n",
       " '▁개': 141,\n",
       " '는데': 142,\n",
       " '▁역시': 143,\n",
       " '▁그리고': 144,\n",
       " '안': 145,\n",
       " '우': 146,\n",
       " '주': 147,\n",
       " '하': 148,\n",
       " '▁뭐': 149,\n",
       " '▁나': 150,\n",
       " '정': 151,\n",
       " '미': 152,\n",
       " '인데': 153,\n",
       " '▁넘': 154,\n",
       " '▁재밌게': 155,\n",
       " '▁또': 156,\n",
       " '적': 157,\n",
       " '▁하는': 158,\n",
       " '??': 159,\n",
       " '▁작품': 160,\n",
       " '▁볼': 161,\n",
       " '던': 162,\n",
       " ';;': 163,\n",
       " \"'\": 164,\n",
       " '중': 165,\n",
       " '▁재미': 166,\n",
       " '▁아깝다': 167,\n",
       " '1': 168,\n",
       " '▁마지막': 169,\n",
       " '▁별로': 170,\n",
       " '드': 171,\n",
       " '진짜': 172,\n",
       " '▁없고': 173,\n",
       " '상': 174,\n",
       " '▁꼭': 175,\n",
       " '보고': 176,\n",
       " '▁ᄏᄏᄏ': 177,\n",
       " '치': 178,\n",
       " '▁감동': 179,\n",
       " '년': 180,\n",
       " '오': 181,\n",
       " '▁가장': 182,\n",
       " '▁무슨': 183,\n",
       " '▁전': 184,\n",
       " ')': 185,\n",
       " '성': 186,\n",
       " '▁말': 187,\n",
       " '2': 188,\n",
       " '보': 189,\n",
       " '▁보면': 190,\n",
       " '같은': 191,\n",
       " '질': 192,\n",
       " '신': 193,\n",
       " '▁감독': 194,\n",
       " 'ᅲᅲ': 195,\n",
       " '전': 196,\n",
       " '것': 197,\n",
       " '세': 198,\n",
       " '마': 199,\n",
       " '용': 200,\n",
       " '라고': 201,\n",
       " '개': 202,\n",
       " '그': 203,\n",
       " '▁하나': 204,\n",
       " '▁지금': 205,\n",
       " '▁만든': 206,\n",
       " '▁오': 207,\n",
       " '했다': 208,\n",
       " '▁이야기': 209,\n",
       " '부': 210,\n",
       " '▁저': 211,\n",
       " 'ᄏᄏᄏ': 212,\n",
       " '분': 213,\n",
       " '내': 214,\n",
       " '▁다른': 215,\n",
       " '좀': 216,\n",
       " '급': 217,\n",
       " '장': 218,\n",
       " '▁ᄏ': 219,\n",
       " '너무': 220,\n",
       " '▁별': 221,\n",
       " '▁중': 222,\n",
       " '입니다': 223,\n",
       " '하지': 224,\n",
       " '비': 225,\n",
       " '▁ᅲᅲ': 226,\n",
       " '▁액션': 227,\n",
       " '▁영화다': 228,\n",
       " '말': 229,\n",
       " ';': 230,\n",
       " '▁같은': 231,\n",
       " '▁말이': 232,\n",
       " '이나': 233,\n",
       " '화': 234,\n",
       " '된': 235,\n",
       " 'ᅲ': 236,\n",
       " '트': 237,\n",
       " '라는': 238,\n",
       " '소': 239,\n",
       " \"▁'\": 240,\n",
       " '▁할': 241,\n",
       " '▁생각': 242,\n",
       " '▁끝까지': 243,\n",
       " '정말': 244,\n",
       " '▁명작': 245,\n",
       " '적인': 246,\n",
       " '습니다': 247,\n",
       " '더': 248,\n",
       " '▁한국': 249,\n",
       " '때': 250,\n",
       " '▁연출': 251,\n",
       " '란': 252,\n",
       " '▁ᅳᅳ': 253,\n",
       " '▁보': 254,\n",
       " '씨': 255,\n",
       " '▁지': 256,\n",
       " '▁와': 257,\n",
       " '처럼': 258,\n",
       " '부터': 259,\n",
       " '▁느낌': 260,\n",
       " '▁솔직히': 261,\n",
       " '▁시간': 262,\n",
       " '▁전혀': 263,\n",
       " '걸': 264,\n",
       " '▁하지만': 265,\n",
       " '▁하': 266,\n",
       " '려': 267,\n",
       " '▁아주': 268,\n",
       " '선': 269,\n",
       " '▁배우': 270,\n",
       " '물': 271,\n",
       " '군': 272,\n",
       " '원': 273,\n",
       " '▁같다': 274,\n",
       " '▁좋다': 275,\n",
       " '▁처음': 276,\n",
       " '▁대한': 277,\n",
       " '에게': 278,\n",
       " '3': 279,\n",
       " '드라마': 280,\n",
       " '히': 281,\n",
       " '없는': 282,\n",
       " '▁있다': 283,\n",
       " '▁주': 284,\n",
       " '▁내용': 285,\n",
       " '(': 286,\n",
       " '▁뭔가': 287,\n",
       " '▁사랑': 288,\n",
       " '▁때': 289,\n",
       " 'ᅳᅳ': 290,\n",
       " '영': 291,\n",
       " '▁아니': 292,\n",
       " '호': 293,\n",
       " '▁봤다': 294,\n",
       " '▁봐도': 295,\n",
       " '한다': 296,\n",
       " '▁많은': 297,\n",
       " '편': 298,\n",
       " '▁사': 299,\n",
       " '▁ᄒᄒ': 300,\n",
       " '있는': 301,\n",
       " '▁재밌다': 302,\n",
       " '▁굿': 303,\n",
       " '인지': 304,\n",
       " '무': 305,\n",
       " '본': 306,\n",
       " '▁짱': 307,\n",
       " '▁재미있게': 308,\n",
       " '▁내내': 309,\n",
       " '▁기대': 310,\n",
       " '인가': 311,\n",
       " '때문에': 312,\n",
       " '유': 313,\n",
       " '▁모두': 314,\n",
       " '러': 315,\n",
       " '조': 316,\n",
       " '▁내용이': 317,\n",
       " '▁않고': 318,\n",
       " '▁일본': 319,\n",
       " '두': 320,\n",
       " '▁어떻게': 321,\n",
       " '▁돈': 322,\n",
       " '▁대박': 323,\n",
       " '▁정': 324,\n",
       " '▁비': 325,\n",
       " '▁최악의': 326,\n",
       " '▁이걸': 327,\n",
       " '▁배우들': 328,\n",
       " '-': 329,\n",
       " '▁계속': 330,\n",
       " '▁하고': 331,\n",
       " '긴': 332,\n",
       " '▁최악': 333,\n",
       " '▁두': 334,\n",
       " '면서': 335,\n",
       " '동': 336,\n",
       " ',,': 337,\n",
       " '이랑': 338,\n",
       " '▁코미디': 339,\n",
       " '며': 340,\n",
       " '▁4': 341,\n",
       " '▁줄': 342,\n",
       " '▁제': 343,\n",
       " '감': 344,\n",
       " '▁유': 345,\n",
       " '단': 346,\n",
       " '당': 347,\n",
       " '▁일': 348,\n",
       " '식': 349,\n",
       " '류': 350,\n",
       " '▁그래도': 351,\n",
       " '▁나오는': 352,\n",
       " '들의': 353,\n",
       " '▁아니라': 354,\n",
       " '▁대': 355,\n",
       " '합니다': 356,\n",
       " '▁거': 357,\n",
       " '줄': 358,\n",
       " '▁해': 359,\n",
       " '▁주인공': 360,\n",
       " '▁재미없다': 361,\n",
       " '어요': 362,\n",
       " '제': 363,\n",
       " '▁아니다': 364,\n",
       " '▁듯': 365,\n",
       " '▁한번': 366,\n",
       " '▁딱': 367,\n",
       " '타': 368,\n",
       " '하지만': 369,\n",
       " '▁여자': 370,\n",
       " '▁연기가': 371,\n",
       " '길': 372,\n",
       " '▁모든': 373,\n",
       " '▁사람': 374,\n",
       " '명': 375,\n",
       " '남': 376,\n",
       " '▁5': 377,\n",
       " '▁아름다운': 378,\n",
       " '▁알': 379,\n",
       " '▁후': 380,\n",
       " '노': 381,\n",
       " '▁아닌': 382,\n",
       " '▁만드는': 383,\n",
       " '▁제일': 384,\n",
       " '디': 385,\n",
       " '위': 386,\n",
       " '▁그런': 387,\n",
       " '▁근데': 388,\n",
       " '하면': 389,\n",
       " '잘': 390,\n",
       " '했던': 391,\n",
       " '난': 392,\n",
       " '▁7': 393,\n",
       " '점도': 394,\n",
       " '▁않은': 395,\n",
       " '린': 396,\n",
       " '없이': 397,\n",
       " '▁보기': 398,\n",
       " '▁어': 399,\n",
       " '이라': 400,\n",
       " '모': 401,\n",
       " '▁감독이': 402,\n",
       " '▁연기도': 403,\n",
       " '엔': 404,\n",
       " '▁나는': 405,\n",
       " '심': 406,\n",
       " '연': 407,\n",
       " '연기': 408,\n",
       " '▁조': 409,\n",
       " '살': 410,\n",
       " '▁제대로': 411,\n",
       " '▁보다가': 412,\n",
       " '이라는': 413,\n",
       " 'ᄒᄒ': 414,\n",
       " '▁스토리가': 415,\n",
       " '▁내용도': 416,\n",
       " '▁8': 417,\n",
       " '▁이영화': 418,\n",
       " '▁이상': 419,\n",
       " '▁않는': 420,\n",
       " '▁김': 421,\n",
       " '되는': 422,\n",
       " '▁자': 423,\n",
       " '운': 424,\n",
       " '▁스릴러': 425,\n",
       " '▁아니고': 426,\n",
       " '▁미': 427,\n",
       " '▁oo': 428,\n",
       " '까': 429,\n",
       " '구나': 430,\n",
       " '방': 431,\n",
       " '날': 432,\n",
       " '작': 433,\n",
       " '▁절대': 434,\n",
       " '▁요즘': 435,\n",
       " '▁시': 436,\n",
       " '죠': 437,\n",
       " '▁싶다': 438,\n",
       " '으면': 439,\n",
       " '봤는데': 440,\n",
       " '▁b': 441,\n",
       " '울': 442,\n",
       " '▁0': 443,\n",
       " '▁기': 444,\n",
       " '시간': 445,\n",
       " '~~~': 446,\n",
       " '▁영화라고': 447,\n",
       " '▁때문에': 448,\n",
       " '▁무': 449,\n",
       " '▁영화의': 450,\n",
       " '▁보다': 451,\n",
       " '래': 452,\n",
       " '하나': 453,\n",
       " '▁시간이': 454,\n",
       " '년대': 455,\n",
       " '▁반': 456,\n",
       " '준': 457,\n",
       " '▁아직도': 458,\n",
       " '▁막장': 459,\n",
       " '발': 460,\n",
       " '▁애니': 461,\n",
       " '들은': 462,\n",
       " '레': 463,\n",
       " '▁마': 464,\n",
       " '!!!!': 465,\n",
       " '▁이제': 466,\n",
       " '재': 467,\n",
       " '▁9': 468,\n",
       " '▁우리': 469,\n",
       " '▁차라리': 470,\n",
       " '♥': 471,\n",
       " '▁재미도': 472,\n",
       " 'ᄒ': 473,\n",
       " '▁신': 474,\n",
       " '▁나름': 475,\n",
       " '보면': 476,\n",
       " '크': 477,\n",
       " '▁인생': 478,\n",
       " '한테': 479,\n",
       " '짱': 480,\n",
       " '▁재밌어요': 481,\n",
       " '▁엄청': 482,\n",
       " '바': 483,\n",
       " '▁영화입니다': 484,\n",
       " '▁좋았다': 485,\n",
       " '\"\"': 486,\n",
       " '▁주는': 487,\n",
       " '▁특히': 488,\n",
       " '▁여운이': 489,\n",
       " '▁걍': 490,\n",
       " '후': 491,\n",
       " '▁보면서': 492,\n",
       " '▁아까운': 493,\n",
       " '▁도대체': 494,\n",
       " '▁진심': 495,\n",
       " '▁장면': 496,\n",
       " '지는': 497,\n",
       " '▁수준': 498,\n",
       " '▁반전': 499,\n",
       " '민': 500,\n",
       " '▁스토리도': 501,\n",
       " '이란': 502,\n",
       " '었다': 503,\n",
       " '▁되는': 504,\n",
       " '▁좋아': 505,\n",
       " '▁남자': 506,\n",
       " '되': 507,\n",
       " '▁전개': 508,\n",
       " '10': 509,\n",
       " 'ᅳ': 510,\n",
       " '번': 511,\n",
       " '점은': 512,\n",
       " '▁감독의': 513,\n",
       " '루': 514,\n",
       " '▁이해': 515,\n",
       " '▁오랜만에': 516,\n",
       " '▁나도': 517,\n",
       " '▁추천': 518,\n",
       " '하네요': 519,\n",
       " '님': 520,\n",
       " '이고': 521,\n",
       " '라도': 522,\n",
       " '▁마지막에': 523,\n",
       " '르': 524,\n",
       " '카': 525,\n",
       " '다니': 526,\n",
       " '▁남는': 527,\n",
       " '▁뻔한': 528,\n",
       " '▁조금': 529,\n",
       " '없다': 530,\n",
       " '희': 531,\n",
       " '인듯': 532,\n",
       " '적으로': 533,\n",
       " '간': 534,\n",
       " '박': 535,\n",
       " '점이': 536,\n",
       " '▁공포': 537,\n",
       " '현': 538,\n",
       " 'd': 539,\n",
       " '▁없음': 540,\n",
       " '▁나온': 541,\n",
       " '▁봤습니다': 542,\n",
       " '▁사람들': 543,\n",
       " '▁만들어': 544,\n",
       " '▁ᄏᄏᄏᄏ': 545,\n",
       " '저': 546,\n",
       " '▁좋고': 547,\n",
       " '애': 548,\n",
       " '▁좋아하는': 549,\n",
       " '▁건': 550,\n",
       " '이라고': 551,\n",
       " '▁영화에': 552,\n",
       " '생': 553,\n",
       " '산': 554,\n",
       " '▁실망': 555,\n",
       " '지도': 556,\n",
       " '▁애': 557,\n",
       " '▁구': 558,\n",
       " '파': 559,\n",
       " '워': 560,\n",
       " '▁소': 561,\n",
       " '???': 562,\n",
       " '▁음악': 563,\n",
       " '▁tv': 564,\n",
       " '버': 565,\n",
       " '▁어떤': 566,\n",
       " '▁지루하고': 567,\n",
       " '왜': 568,\n",
       " '▁ooo': 569,\n",
       " '▁지루한': 570,\n",
       " '▁생각이': 571,\n",
       " '▁이해가': 572,\n",
       " '니까': 573,\n",
       " '▁시작': 574,\n",
       " '판': 575,\n",
       " '▁멋진': 576,\n",
       " '▁남': 577,\n",
       " '▁cg': 578,\n",
       " '키': 579,\n",
       " '차': 580,\n",
       " '▁봤던': 581,\n",
       " '▁결말': 582,\n",
       " '▁없이': 583,\n",
       " '생각': 584,\n",
       " '태': 585,\n",
       " '보는': 586,\n",
       " '▁^^': 587,\n",
       " '했는데': 588,\n",
       " '▁개봉': 589,\n",
       " '▁함께': 590,\n",
       " '들을': 591,\n",
       " '따': 592,\n",
       " 'ᅮᅮ': 593,\n",
       " '▁속': 594,\n",
       " '그냥': 595,\n",
       " '져': 596,\n",
       " '하네': 597,\n",
       " '▁ᅲ': 598,\n",
       " '▁부': 599,\n",
       " '▁최고다': 600,\n",
       " '▁사람이': 601,\n",
       " '▁배우들의': 602,\n",
       " '피': 603,\n",
       " '▁괜찮은': 604,\n",
       " '못': 605,\n",
       " '는게': 606,\n",
       " '즈': 607,\n",
       " '▁개인적으로': 608,\n",
       " '▁뭘': 609,\n",
       " '림': 610,\n",
       " '▁눈물': 611,\n",
       " '만큼': 612,\n",
       " '관': 613,\n",
       " '▁모': 614,\n",
       " '▁보는내내': 615,\n",
       " '▁재미가': 616,\n",
       " '터': 617,\n",
       " '속': 618,\n",
       " 'ᄏᄏᄏᄏ': 619,\n",
       " '▁못한': 620,\n",
       " '4': 621,\n",
       " '밖에': 622,\n",
       " '경': 623,\n",
       " '▁애니메이션': 624,\n",
       " '▁오늘': 625,\n",
       " '프': 626,\n",
       " '▁그저': 627,\n",
       " '반': 628,\n",
       " '▁얼마나': 629,\n",
       " '든': 630,\n",
       " '▁싶은': 631,\n",
       " '코': 632,\n",
       " '▁보세요': 633,\n",
       " '▁영화로': 634,\n",
       " '실': 635,\n",
       " '▁되': 636,\n",
       " '다가': 637,\n",
       " '스토리': 638,\n",
       " '▁시리즈': 639,\n",
       " '▁아무리': 640,\n",
       " '▁20': 641,\n",
       " '참': 642,\n",
       " '▁재밌는': 643,\n",
       " '▁가슴': 644,\n",
       " '▁스': 645,\n",
       " '/': 646,\n",
       " '~!': 647,\n",
       " ',,,': 648,\n",
       " '▁눈': 649,\n",
       " '▁큰': 650,\n",
       " '▁걸': 651,\n",
       " '▁노': 652,\n",
       " '▁우': 653,\n",
       " '▁지루하다': 654,\n",
       " '배우': 655,\n",
       " '▁\"': 656,\n",
       " '▁기억에': 657,\n",
       " '겨': 658,\n",
       " '▁피': 659,\n",
       " '▁몇': 660,\n",
       " '▁안되는': 661,\n",
       " '해도': 662,\n",
       " '토': 663,\n",
       " '있다': 664,\n",
       " '▁원작': 665,\n",
       " '해요': 666,\n",
       " '▁졸작': 667,\n",
       " '국': 668,\n",
       " '5': 669,\n",
       " '건지': 670,\n",
       " '▁뭔': 671,\n",
       " '이런': 672,\n",
       " '▁공포영화': 673,\n",
       " '배': 674,\n",
       " '▁정도': 675,\n",
       " '▁될': 676,\n",
       " '▁공감': 677,\n",
       " '중에': 678,\n",
       " '▁연기는': 679,\n",
       " '▁세': 680,\n",
       " '사람': 681,\n",
       " '▁매우': 682,\n",
       " '▁우리나라': 683,\n",
       " '공': 684,\n",
       " '최고': 685,\n",
       " '티': 686,\n",
       " '▁된': 687,\n",
       " '▁막': 688,\n",
       " '▁제발': 689,\n",
       " '▁박': 690,\n",
       " '는거': 691,\n",
       " '▁기억': 692,\n",
       " '▁그렇게': 693,\n",
       " '작품': 694,\n",
       " '▁알바': 695,\n",
       " '회': 696,\n",
       " '금': 697,\n",
       " '▁인간': 698,\n",
       " '▁장': 699,\n",
       " '▁6': 700,\n",
       " '망': 701,\n",
       " '더라': 702,\n",
       " '문': 703,\n",
       " '▁연기력': 704,\n",
       " '▁같이': 705,\n",
       " '불': 706,\n",
       " '▁좋아요': 707,\n",
       " '▁영': 708,\n",
       " '평점': 709,\n",
       " 's': 710,\n",
       " '▁것이': 711,\n",
       " '▁모르겠다': 712,\n",
       " '▁몰입도': 713,\n",
       " '는지': 714,\n",
       " '▁있고': 715,\n",
       " '▁쓰': 716,\n",
       " '하기': 717,\n",
       " '▁캐릭터': 718,\n",
       " '▁원': 719,\n",
       " '▁기분': 720,\n",
       " '이지만': 721,\n",
       " '▁\"\"': 722,\n",
       " '▁봤어요': 723,\n",
       " '매': 724,\n",
       " '▁극장에서': 725,\n",
       " '▁바': 726,\n",
       " '▁하나도': 727,\n",
       " '▁대사': 728,\n",
       " ';;;': 729,\n",
       " '정도': 730,\n",
       " '▁위해': 731,\n",
       " '감독': 732,\n",
       " '으': 733,\n",
       " '▁않는다': 734,\n",
       " '쓰레기': 735,\n",
       " '▁수작': 736,\n",
       " '▁너무나': 737,\n",
       " '▁마음이': 738,\n",
       " '▁만들': 739,\n",
       " '▁위한': 740,\n",
       " '극': 741,\n",
       " '형': 742,\n",
       " '▁날': 743,\n",
       " '▁강': 744,\n",
       " '▁결국': 745,\n",
       " '갈': 746,\n",
       " '병': 747,\n",
       " '담': 748,\n",
       " '▁표현': 749,\n",
       " '▁긴장감': 750,\n",
       " '▁불': 751,\n",
       " '던데': 752,\n",
       " '▁재밌음': 753,\n",
       " '▁지루함': 754,\n",
       " '양': 755,\n",
       " '였다': 756,\n",
       " '사랑': 757,\n",
       " '▁미국': 758,\n",
       " '▁한다': 759,\n",
       " '▁끝': 760,\n",
       " '하면서': 761,\n",
       " '그리고': 762,\n",
       " '▁ᄒ': 763,\n",
       " '▁존나': 764,\n",
       " '쳐': 765,\n",
       " '하며': 766,\n",
       " '▁보게': 767,\n",
       " '▁시나리오': 768,\n",
       " '▁준': 769,\n",
       " '있': 770,\n",
       " '탄': 771,\n",
       " '것도': 772,\n",
       " '▁한국영화': 773,\n",
       " '했음': 774,\n",
       " 'ᅮ': 775,\n",
       " '녀': 776,\n",
       " '▁배': 777,\n",
       " '▁살': 778,\n",
       " '겠다': 779,\n",
       " '▁대체': 780,\n",
       " '▁소재': 781,\n",
       " '▁느낌이': 782,\n",
       " '구만': 783,\n",
       " '▁심': 784,\n",
       " '등': 785,\n",
       " '초': 786,\n",
       " '▁눈물이': 787,\n",
       " '▁성': 788,\n",
       " '▁로맨스': 789,\n",
       " '▁재밌': 790,\n",
       " '김': 791,\n",
       " '▁강추': 792,\n",
       " '▁내용은': 793,\n",
       " '달': 794,\n",
       " '이네': 795,\n",
       " '▁보지': 796,\n",
       " '▁훨씬': 797,\n",
       " '뿐': 798,\n",
       " '▁영화네요': 799,\n",
       " '▁미친': 800,\n",
       " '▁ost': 801,\n",
       " '6': 802,\n",
       " '악': 803,\n",
       " '대로': 804,\n",
       " '꺼': 805,\n",
       " '▁갈수록': 806,\n",
       " '이야': 807,\n",
       " '었는데': 808,\n",
       " '▁발': 809,\n",
       " '역': 810,\n",
       " '▁문제': 811,\n",
       " '▁필요': 812,\n",
       " '▁억지': 813,\n",
       " '▁누가': 814,\n",
       " '▁영상': 815,\n",
       " '▁그나마': 816,\n",
       " '▁가슴이': 817,\n",
       " '종': 818,\n",
       " '점을': 819,\n",
       " '▁알고': 820,\n",
       " '▁음': 821,\n",
       " '▁밖에': 822,\n",
       " '이네요': 823,\n",
       " '▁꽤': 824,\n",
       " '▁단': 825,\n",
       " '봄': 826,\n",
       " '포': 827,\n",
       " '▁결말이': 828,\n",
       " '거야': 829,\n",
       " '▁파': 830,\n",
       " '주는': 831,\n",
       " '강': 832,\n",
       " '▁기억이': 833,\n",
       " '7': 834,\n",
       " '계': 835,\n",
       " '▁사실': 836,\n",
       " '려고': 837,\n",
       " '▁재미없음': 838,\n",
       " '▁ᅮᅮ': 839,\n",
       " '어서': 840,\n",
       " '▁입': 841,\n",
       " '▁그만': 842,\n",
       " '▁더빙': 843,\n",
       " '▁머': 844,\n",
       " '감동': 845,\n",
       " '넘': 846,\n",
       " '▁치': 847,\n",
       " '체': 848,\n",
       " '▁여': 849,\n",
       " '▁초': 850,\n",
       " '▁보여주는': 851,\n",
       " '▁않': 852,\n",
       " '▁그래서': 853,\n",
       " '▁어설픈': 854,\n",
       " '이었다': 855,\n",
       " '▁예': 856,\n",
       " '▁재밌고': 857,\n",
       " '▁연기를': 858,\n",
       " '▁들': 859,\n",
       " '▁없어': 860,\n",
       " '▁않다': 861,\n",
       " '▁쓰레기영화': 862,\n",
       " '군요': 863,\n",
       " '봐도': 864,\n",
       " '하는데': 865,\n",
       " '▁노잼': 866,\n",
       " '볼': 867,\n",
       " '▁울': 868,\n",
       " '+': 869,\n",
       " '너': 870,\n",
       " '봐': 871,\n",
       " '▁진': 872,\n",
       " 't': 873,\n",
       " '8': 874,\n",
       " '▁다들': 875,\n",
       " '완전': 876,\n",
       " '▁주고': 877,\n",
       " '글': 878,\n",
       " '▁욕': 879,\n",
       " '▁없었다': 880,\n",
       " '▁걸작': 881,\n",
       " '▁허': 882,\n",
       " '▁시즌': 883,\n",
       " '▁아쉽다': 884,\n",
       " '▁매력': 885,\n",
       " '▁캐스팅': 886,\n",
       " '▁설정': 887,\n",
       " '▁재': 888,\n",
       " '액션': 889,\n",
       " '▁만화': 890,\n",
       " '▁간만에': 891,\n",
       " '▁영화관에서': 892,\n",
       " '내용': 893,\n",
       " '▁게': 894,\n",
       " '입': 895,\n",
       " '▁선': 896,\n",
       " '▁전쟁': 897,\n",
       " '▁사람들이': 898,\n",
       " '▁감동도': 899,\n",
       " '▁가족': 900,\n",
       " '▁감동이': 901,\n",
       " '뭐': 902,\n",
       " '▁-': 903,\n",
       " '▁잔잔한': 904,\n",
       " '리는': 905,\n",
       " '론': 906,\n",
       " '▁good': 907,\n",
       " '▁애들': 908,\n",
       " '▁차': 909,\n",
       " '▁좋은데': 910,\n",
       " '▁것도': 911,\n",
       " '▁영화중': 912,\n",
       " '▁당시': 913,\n",
       " '▁있을': 914,\n",
       " '▁것을': 915,\n",
       " '봤다': 916,\n",
       " '이거': 917,\n",
       " '▁노래': 918,\n",
       " '▁30': 919,\n",
       " '느낌': 920,\n",
       " '점대': 921,\n",
       " '▁매': 922,\n",
       " '▁중국': 923,\n",
       " '▁감동적인': 924,\n",
       " '인줄': 925,\n",
       " '▁공': 926,\n",
       " '▁제목': 927,\n",
       " '▁코믹': 928,\n",
       " '년이': 929,\n",
       " '이건': 930,\n",
       " '▁바로': 931,\n",
       " '▁영화였다': 932,\n",
       " '▁아님': 933,\n",
       " '▁같아요': 934,\n",
       " '▁발연기': 935,\n",
       " '▁주연': 936,\n",
       " '▁있을까': 937,\n",
       " '▁현실': 938,\n",
       " '장면': 939,\n",
       " '▁대해': 940,\n",
       " 'o': 941,\n",
       " '▁생각을': 942,\n",
       " '▁망': 943,\n",
       " '▁재미있는': 944,\n",
       " '기도': 945,\n",
       " '▁생각보다': 946,\n",
       " '▁진정한': 947,\n",
       " '▁너': 948,\n",
       " '▁다큐': 949,\n",
       " '머': 950,\n",
       " '▁약간': 951,\n",
       " '▁상당히': 952,\n",
       " '▁별점': 953,\n",
       " '잼': 954,\n",
       " '▁야': 955,\n",
       " '\"': 956,\n",
       " '온': 957,\n",
       " '▁동': 958,\n",
       " '▁준다': 959,\n",
       " '▁뿐': 960,\n",
       " '집': 961,\n",
       " '했지만': 962,\n",
       " '▁있': 963,\n",
       " '▁상': 964,\n",
       " '▁재미있다': 965,\n",
       " '영화는': 966,\n",
       " '▁그대로': 967,\n",
       " '수준': 968,\n",
       " '▁네이버': 969,\n",
       " '기는': 970,\n",
       " '▁작가': 971,\n",
       " '▁자기': 972,\n",
       " '알': 973,\n",
       " '▁후회': 974,\n",
       " '▁스토리는': 975,\n",
       " '▁장난': 976,\n",
       " '통': 977,\n",
       " '▁적': 978,\n",
       " '예': 979,\n",
       " '에도': 980,\n",
       " '▁찾아': 981,\n",
       " '움': 982,\n",
       " '▁같은데': 983,\n",
       " '▁중간에': 984,\n",
       " '▁배우들이': 985,\n",
       " '업': 986,\n",
       " '▁감동을': 987,\n",
       " '▁판타지': 988,\n",
       " '▁했는데': 989,\n",
       " '▁모습': 990,\n",
       " '없고': 991,\n",
       " '이야기': 992,\n",
       " '▁거의': 993,\n",
       " '▁정도로': 994,\n",
       " '▁점수': 995,\n",
       " '▁행복': 996,\n",
       " '색': 997,\n",
       " '이지': 998,\n",
       " '해야': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6729c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁아', '▁더빙', '..', '▁진짜', '▁짜증나', '네요', '▁목소리', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#print(word_index)\n",
    "#print(index_word)\n",
    "\n",
    "for sequence in tensor:\n",
    "    print([index_word[word]for word in sequence])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a81411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:07<00:00, 18487.73it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:02<00:00, 19181.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126162, 40)\n",
      "(126162,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt')\n",
    "\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s)\n",
    "\n",
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da0a06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "efb2f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 4s 8ms/step - loss: 0.6258 - accuracy: 0.6230 - val_loss: 0.4420 - val_accuracy: 0.8273\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3632 - accuracy: 0.8527 - val_loss: 0.3463 - val_accuracy: 0.8515\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2989 - accuracy: 0.8803 - val_loss: 0.3510 - val_accuracy: 0.8536\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2778 - accuracy: 0.8883 - val_loss: 0.3472 - val_accuracy: 0.8529\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0547bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/1536 - 3s - loss: 0.3569 - accuracy: 0.8458\n",
      "[0.3568832576274872, 0.8457579016685486]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42315978",
   "metadata": {},
   "source": [
    "# KoNLPy 형태소 분석기 사용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f7d11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab : 한국어 형태소 분석기\n",
    "tokenizer = Mecab()\n",
    "maxlen = 40\n",
    "\n",
    "def load_data(train_data, test_data, s,num_words=10000):\n",
    "    # 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document'])\n",
    "    test_data = test_data.drop_duplicates(subset=['document'])\n",
    "\n",
    "    # Nan 결측치 제거\n",
    "    # https://wikidocs.net/153202\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    # 토큰화, 형태소에 대한 구문 분석\n",
    "    # https://konlpy.org/ko/v0.6.0/api/konlpy.tag/\n",
    "    train_tokens = [tokenizer.morphs(sentence) for sentence in train_data['document']]\n",
    "    test_tokens =  [tokenizer.morphs(sentence) for sentence in test_data['document']]\n",
    "\n",
    "    # Use SentencePiece\n",
    "    #train_tokens, word_to_index, _ = sp_tokenize(s, train_data['document'])\n",
    "    #test_tokens, _, _ = sp_tokenize(s, test_data['document'])\n",
    "    \n",
    "    # word_to_index 구성\n",
    "    # Counter를 사용하여 많이 사용된 num_words 개의 단어 vocab 만들기\n",
    "    word_to_index = {}\n",
    "    # 2-d list를 1-d list로 변환\n",
    "    words = np.concatenate(train_tokens).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(num_words-4)\n",
    "    vocab = [key for key, _ in counter]\n",
    "    # 미리 정의된 토큰 4개를 제외하고 단어 인덱스 부여\n",
    "    word_to_index = {word:index+4 for index, word in enumerate(vocab)}\n",
    "\n",
    "    word_to_index['<pad>']=  0# 패딩용 단어\n",
    "    word_to_index['<s>']=  1# 문장의 시작지점\n",
    "    word_to_index['</s>']= 2\n",
    "    word_to_index['<unk>']= 3\n",
    "    \n",
    "    # text string to vocab index string\n",
    "    # X_train의 상위 10000개의 단어만 단어에 등록되었기 때문에, X_train에도 <UNK>토큰이 발생할 수 있다.\n",
    "    X_train = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in train_token] for train_token in train_tokens]\n",
    "    X_test = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in test_token] for test_token in test_tokens]\n",
    "    \n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "    \n",
    "    return X_train, np.array(train_data['label']), X_test, test_data['label'], word_to_index\n",
    "\n",
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt')\n",
    "\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0566105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126162, 40)\n",
      "(126162,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3f2dfd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5e75dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 4s 8ms/step - loss: 0.5291 - accuracy: 0.7546 - val_loss: 0.3789 - val_accuracy: 0.8363\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3469 - accuracy: 0.8534 - val_loss: 0.3466 - val_accuracy: 0.8506\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3158 - accuracy: 0.8683 - val_loss: 0.3390 - val_accuracy: 0.8558\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2993 - accuracy: 0.8758 - val_loss: 0.3383 - val_accuracy: 0.8561\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2865 - accuracy: 0.8808 - val_loss: 0.3409 - val_accuracy: 0.8562\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2757 - accuracy: 0.8861 - val_loss: 0.3483 - val_accuracy: 0.8545\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e044a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/1536 - 3s - loss: 0.3475 - accuracy: 0.8495\n",
      "[0.34745103120803833, 0.8494811654090881]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0123504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OOV% : 55.11\n",
      "Test  OOV% : 55.26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def oov_ratio(encoded, unk_id=0):\n",
    "    total = sum(len(seq) for seq in encoded)\n",
    "    unk   = sum((tok == unk_id) for seq in encoded for tok in seq)\n",
    "    return round(100 * unk / total, 2)\n",
    "\n",
    "print(\"Train OOV% :\", oov_ratio(X_train))\n",
    "print(\"Test  OOV% :\", oov_ratio(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d1780",
   "metadata": {},
   "source": [
    "# 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3629b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data, test_data, tokenizer, max_len=40, num_words=10000, use_sentence_piece=True):\n",
    "    # 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document'])\n",
    "    test_data = test_data.drop_duplicates(subset=['document'])\n",
    "\n",
    "    # Nan 결측치 제거\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    if use_sentence_piece:\n",
    "        train_tokens, word_to_index, _ = sp_tokenize(tokenizer, train_data['document'], maxlen=max_len)\n",
    "        test_tokens, _, _ = sp_tokenize(tokenizer, test_data['document'], maxlen=max_len)\n",
    "        \n",
    "        return train_tokens, np.array(train_data['label']), test_tokens, test_data['label'], word_to_index\n",
    "    else:\n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '.']\n",
    "\n",
    "        # 토큰화, 형태소에 대한 구문 분석\n",
    "        train_tokens = [tokenizer.morphs(sentence) for sentence in train_data['document']]\n",
    "        test_tokens =  [tokenizer.morphs(sentence) for sentence in test_data['document']]\n",
    "        \n",
    "        # 불용어 제거\n",
    "        train_tokens = [[token for token in sentence if token not in stopwords] for sentence in train_tokens]\n",
    "        test_tokens =  [[token for token in sentence if token not in stopwords] for sentence in test_tokens]\n",
    "\n",
    "        \n",
    "        # word_to_index 구성\n",
    "        word_to_index = {}\n",
    "        words = np.concatenate(train_tokens).tolist()\n",
    "        counter = Counter(words)\n",
    "        counter = counter.most_common(num_words-4)\n",
    "        vocab = [key for key, _ in counter]\n",
    "        word_to_index = {word:index+4 for index, word in enumerate(vocab)}\n",
    "\n",
    "        word_to_index['<pad>']=  0# 패딩용 단어\n",
    "        word_to_index['<s>']=  1# 문장의 시작지점\n",
    "        word_to_index['</s>']= 2\n",
    "        word_to_index['<unk>']= 3\n",
    "    \n",
    "        X_train = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in train_token] for train_token in train_tokens]\n",
    "        X_test = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in test_token] for test_token in test_tokens]\n",
    "\n",
    "        X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "        X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "    \n",
    "        return X_train, np.array(train_data['label']), X_test, test_data['label'], word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de8eb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt\n",
    "\n",
    "def review_model(train_path, test_path, model, tokenizer='sp_ke_park', vocab_size = 16000, val_len=20000, epochs=20):\n",
    "    assert tokenizer in ['sp_ke_park','se', 'mecab', 'hannanum', 'kkma', 'komoran', 'okt']\n",
    "    \n",
    "    train_data = pd.read_table(train_path)\n",
    "    test_data = pd.read_table(test_path)\n",
    "    \n",
    "    train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "    test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "    if tokenizer == 'sp_ke_park':\n",
    "        temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "        with open(temp_file, 'w') as f:\n",
    "            for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "                f.write(str(row) + '\\n')\n",
    "\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            '--input={} --model_prefix=korean_spm --vocab_size={} --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(temp_file, vocab_size)    \n",
    "        )\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.load(\"korean_spm.model\")\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer)\n",
    "    elif tokenizer == 'se':\n",
    "        clean_path = \"spm_input.txt\"\n",
    "\n",
    "        with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in train_data['document']:\n",
    "                f.write(str(line).strip() + \"\\n\")\n",
    "\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            '--input={} --model_prefix=korean_spm --vocab_size={}  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(clean_path, vocab_size)    \n",
    "        )\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.load(\"korean_spm.model\")\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer)\n",
    "    elif tokenizer == 'mecab':\n",
    "        tokenizer = Mecab()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'hannanum':\n",
    "        tokenizer = Hannanum()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'kkma':\n",
    "        tokenizer = Kkma()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'komoran':\n",
    "        tokenizer = Komoran()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'okt':\n",
    "        tokenizer = Okt()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    \n",
    "    \n",
    "    #print(\"Train OOV% :\", oov_ratio(X_train))\n",
    "    #print(\"Test  OOV% :\", oov_ratio(X_test))\n",
    "    \n",
    "    X_val = X_train[:val_len]   \n",
    "    y_val = y_train[:val_len]\n",
    "\n",
    "    partial_x_train = X_train[val_len:]  \n",
    "    partial_y_train = y_train[val_len:]\n",
    "    \n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               patience=2,\n",
    "                               restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(partial_x_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop, checkpoint])\n",
    "    results = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(results)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "598c5643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "47154f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=spm_input.txt --model_prefix=korean_spm --vocab_size=16000  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_input.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: spm_input.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 150000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430599\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1686\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 150000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308597 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 150000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357031\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 357031 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157511 obj=15.4277 num_tokens=841362 num_tokens/piece=5.34161\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145725 obj=14.3586 num_tokens=846836 num_tokens/piece=5.81119\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109233 obj=14.4548 num_tokens=882029 num_tokens/piece=8.07475\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109051 obj=14.3996 num_tokens=882763 num_tokens/piece=8.09496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81784 obj=14.6341 num_tokens=926725 num_tokens/piece=11.3314\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81776 obj=14.5731 num_tokens=926844 num_tokens/piece=11.3339\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61332 obj=14.8307 num_tokens=968598 num_tokens/piece=15.7927\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61332 obj=14.7702 num_tokens=968692 num_tokens/piece=15.7942\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45999 obj=15.0586 num_tokens=1013498 num_tokens/piece=22.033\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45999 obj=14.9947 num_tokens=1013492 num_tokens/piece=22.0329\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34499 obj=15.3164 num_tokens=1059614 num_tokens/piece=30.7143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34499 obj=15.2502 num_tokens=1059627 num_tokens/piece=30.7147\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25874 obj=15.6061 num_tokens=1107398 num_tokens/piece=42.7996\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25874 obj=15.534 num_tokens=1107416 num_tokens/piece=42.8003\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19405 obj=15.9185 num_tokens=1157734 num_tokens/piece=59.6616\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19405 obj=15.8384 num_tokens=1157755 num_tokens/piece=59.6627\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=15.9723 num_tokens=1175362 num_tokens/piece=66.7819\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=15.9443 num_tokens=1175368 num_tokens/piece=66.7823\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n",
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:03<00:00, 46191.42it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:00<00:00, 49299.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OOV% : 59.16\n",
      "Test  OOV% : 59.1\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 3s 8ms/step - loss: 0.5720 - accuracy: 0.6665 - val_loss: 0.3832 - val_accuracy: 0.8387\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3351 - accuracy: 0.8617 - val_loss: 0.3476 - val_accuracy: 0.8508\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8807 - val_loss: 0.3399 - val_accuracy: 0.8567\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2740 - accuracy: 0.8882 - val_loss: 0.3492 - val_accuracy: 0.8522\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8922 - val_loss: 0.3754 - val_accuracy: 0.8413\n",
      "1536/1536 - 3s - loss: 0.3521 - accuracy: 0.8483\n",
      "[0.3521427512168884, 0.8483214378356934]\n"
     ]
    }
   ],
   "source": [
    "train_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt'\n",
    "test_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt'\n",
    "\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83798db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train OOV% : 62.1\n",
      "Test  OOV% : 62.22\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 3s 7ms/step - loss: 0.5244 - accuracy: 0.7609 - val_loss: 0.3775 - val_accuracy: 0.8414\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3463 - accuracy: 0.8548 - val_loss: 0.3491 - val_accuracy: 0.8496\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.3468 - val_accuracy: 0.8514\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2978 - accuracy: 0.8785 - val_loss: 0.3480 - val_accuracy: 0.8517\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2849 - accuracy: 0.8848 - val_loss: 0.3572 - val_accuracy: 0.8475\n",
      "1536/1536 - 3s - loss: 0.3554 - accuracy: 0.8467\n",
      "[0.35537293553352356, 0.8466938138008118]\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cca5bd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=16000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=14.4669 num_tokens=682473 num_tokens/piece=38.7769\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=14.4447 num_tokens=682479 num_tokens/piece=38.7772\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n",
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:03<00:00, 41239.61it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:01<00:00, 40862.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OOV% : 46.68\n",
      "Test  OOV% : 46.98\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 3s 7ms/step - loss: 0.5553 - accuracy: 0.7035 - val_loss: 0.4253 - val_accuracy: 0.8141\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3929 - accuracy: 0.8298 - val_loss: 0.3940 - val_accuracy: 0.8213\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3699 - accuracy: 0.8413 - val_loss: 0.3932 - val_accuracy: 0.8217\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3607 - accuracy: 0.8451 - val_loss: 0.3887 - val_accuracy: 0.8261\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.3534 - accuracy: 0.8491 - val_loss: 0.3899 - val_accuracy: 0.8252\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8521 - val_loss: 0.3931 - val_accuracy: 0.8242\n",
      "1536/1536 - 3s - loss: 0.3972 - accuracy: 0.8227\n",
      "[0.3971611559391022, 0.8226653337478638]\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'sp_ke_park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2a8f550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"word_vector_dim = 32\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\\nmodel.add(tf.keras.layers.LSTM(16))\\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\nmodel.summary()\\nmodel, history = review_model(train_path, test_path, model, tokenizer = 'hannanum')\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# hannanum은 java.lang.ArrayIndexOutOfBoundsException 예외 발생, 특수문자 때문인 것으로 추정됨.\n",
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'hannanum')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f899c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 너무 오래걸림\\nword_vector_dim = 32\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\\nmodel.add(tf.keras.layers.LSTM(16))\\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\nmodel.summary()\\nmodel, history = review_model(train_path, test_path, model, tokenizer = 'kkma')\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 너무 오래걸림\n",
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'kkma')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "293fcc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=spm_input.txt --model_prefix=korean_spm --vocab_size=24000  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_input.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 24000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: spm_input.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 150000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430599\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1686\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 150000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308597 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 150000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357031\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 357031 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157511 obj=15.4277 num_tokens=841362 num_tokens/piece=5.34161\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145725 obj=14.3586 num_tokens=846836 num_tokens/piece=5.81119\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109233 obj=14.4548 num_tokens=882029 num_tokens/piece=8.07475\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109051 obj=14.3996 num_tokens=882763 num_tokens/piece=8.09496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81784 obj=14.6341 num_tokens=926725 num_tokens/piece=11.3314\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81776 obj=14.5731 num_tokens=926844 num_tokens/piece=11.3339\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61332 obj=14.8307 num_tokens=968598 num_tokens/piece=15.7927\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61332 obj=14.7702 num_tokens=968692 num_tokens/piece=15.7942\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45999 obj=15.0586 num_tokens=1013498 num_tokens/piece=22.033\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45999 obj=14.9947 num_tokens=1013492 num_tokens/piece=22.0329\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34499 obj=15.3164 num_tokens=1059614 num_tokens/piece=30.7143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34499 obj=15.2502 num_tokens=1059627 num_tokens/piece=30.7147\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26400 obj=15.5785 num_tokens=1103885 num_tokens/piece=41.8138\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26400 obj=15.5122 num_tokens=1103905 num_tokens/piece=41.8146\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n",
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:05<00:00, 27420.29it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:01<00:00, 28895.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 7s 13ms/step - loss: 0.6433 - accuracy: 0.5872 - val_loss: 0.4507 - val_accuracy: 0.8080\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.3699 - accuracy: 0.8434 - val_loss: 0.3513 - val_accuracy: 0.8475\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3068 - accuracy: 0.8742 - val_loss: 0.3489 - val_accuracy: 0.8461\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2873 - accuracy: 0.8821 - val_loss: 0.3521 - val_accuracy: 0.8461\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2751 - accuracy: 0.8870 - val_loss: 0.3578 - val_accuracy: 0.8464\n",
      "1536/1536 - 3s - loss: 0.3611 - accuracy: 0.8416\n",
      "[0.36105504631996155, 0.8416480422019958]\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'se', vocab_size=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d1a9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"word_vector_dim = 32\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\\nmodel.add(tf.keras.layers.LSTM(16))\\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\nmodel.summary()\\nmodel, history = review_model(train_path, test_path, model, tokenizer = 'okt')\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'okt')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c50be7",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4b0fc",
   "metadata": {},
   "source": [
    "otk, hannanum, kkma를 진행해보고싶었는데, 속도가 느린건지 잘 돌아가지 않아서 시도해보지 못한 게 아쉽다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
