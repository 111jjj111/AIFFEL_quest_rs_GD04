{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "650f390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d41f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f44e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n",
      "문장의 최단 길이: 10\n",
      "문장의 최장 길이: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "max_len = 150\n",
    "min_len = 10\n",
    "    \n",
    "print(\"Data Size:\", len(raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)\n",
    "\n",
    "cleaned_corpus = list(set(raw)) \n",
    "\n",
    "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4accedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=16000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=14.4669 num_tokens=682473 num_tokens/piece=38.7769\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=14.4447 num_tokens=682479 num_tokens/piece=38.7772\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 535805 May  9 06:58 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 312443 May  9 06:58 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece 모델 학습\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 16000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={} --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69db06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1074, 12, 691, 10, 3212, 12, 304, 41, 4]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b9bfe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 함수 작성\n",
    "from tqdm import tqdm\n",
    "def sp_tokenize(s, corpus, maxlen=150,add_bos=True, add_eos=True): \n",
    "\n",
    "    tensor = []\n",
    "    bos_id = s.bos_id()\n",
    "    eos_id = s.eos_id()\n",
    "    \n",
    "    corpus = corpus.astype(str)\n",
    "\n",
    "    for sen in tqdm(corpus, desc=\"SentencePiece Tokenizing\"):\n",
    "        ids = s.EncodeAsIds(sen)\n",
    "        if add_bos:\n",
    "            ids = [bos_id] + ids\n",
    "        if add_eos:\n",
    "            ids = ids + [eos_id]\n",
    "        tensor.append(ids)\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=maxlen)\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51f9233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리     0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나     1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다     0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정     0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...     1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    file = f.readlines()\n",
    "    column_name = file[0].strip().split('\\t')\n",
    "    data_split = [x.strip().split('\\t')for x in file[1:]]\n",
    "    data = pd.DataFrame(data_split, columns=column_name)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2cdd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(seq):\n",
    "    seq = str(seq).lower()\n",
    "    \n",
    "    slang_map = {\n",
    "        \"ㅅㅂ\": \"시발\",\n",
    "        \"ㅄ\": \"병신\",\n",
    "        \"ㅈㄴ\": \"아주\",\n",
    "        \"ㅂㅅ\": \"병신\",\n",
    "        \"ㅁㅊ\": \"미친\",\n",
    "    }\n",
    "\n",
    "    # 패턴을 하나로 결합\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, slang_map.keys())) + r')\\b'\n",
    "\n",
    "    # 매치된 문자열을 dict에서 찾아서 치환\n",
    "    seq = re.sub(pattern, lambda m: slang_map[m.group()], seq)\n",
    "    \n",
    "    # 'O'만 구성된 단어 중 반복된 것만 욕설로 치환\n",
    "    seq = re.sub(r'\\bO{2,}\\b', '욕설', seq)\n",
    "    \n",
    "    return seq\n",
    "data['document'] = data['document'].astype(str).apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d28ca1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenizing: 100%|██████████| 150000/150000 [00:03<00:00, 41691.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tensor, word_index, index_word = sp_tokenize(s, data.iloc[:]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25935869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '.': 4,\n",
       " '을': 5,\n",
       " '▁': 6,\n",
       " '의': 7,\n",
       " '를': 8,\n",
       " '는': 9,\n",
       " '에': 10,\n",
       " '이': 11,\n",
       " '가': 12,\n",
       " '은': 13,\n",
       " ',': 14,\n",
       " '고': 15,\n",
       " '에서': 16,\n",
       " '▁“': 17,\n",
       " '로': 18,\n",
       " '”': 19,\n",
       " '한': 20,\n",
       " '인': 21,\n",
       " '일': 22,\n",
       " ')': 23,\n",
       " '(': 24,\n",
       " '▁이': 25,\n",
       " '과': 26,\n",
       " '▁있다': 27,\n",
       " '으로': 28,\n",
       " '와': 29,\n",
       " '▁수': 30,\n",
       " '도': 31,\n",
       " '▁밝혔다': 32,\n",
       " '▁말했다': 33,\n",
       " '할': 34,\n",
       " '년': 35,\n",
       " '지': 36,\n",
       " '▁있는': 37,\n",
       " '며': 38,\n",
       " '▁그': 39,\n",
       " '하고': 40,\n",
       " '다': 41,\n",
       " '하는': 42,\n",
       " '했다': 43,\n",
       " '▁그는': 44,\n",
       " '▁전': 45,\n",
       " '▁2': 46,\n",
       " '▁1': 47,\n",
       " '▁대한': 48,\n",
       " '▁위해': 49,\n",
       " '만': 50,\n",
       " '월': 51,\n",
       " '▁전했다': 52,\n",
       " '▁한': 53,\n",
       " '▁미국': 54,\n",
       " '해': 55,\n",
       " '▁이번': 56,\n",
       " '▁3': 57,\n",
       " '기': 58,\n",
       " '▁지난': 59,\n",
       " '현지시간': 60,\n",
       " '▁중': 61,\n",
       " '▁대해': 62,\n",
       " '자': 63,\n",
       " '\"': 64,\n",
       " '된': 65,\n",
       " '▁미': 66,\n",
       " '▁것으로': 67,\n",
       " '▁‘': 68,\n",
       " '에게': 69,\n",
       " '스': 70,\n",
       " '▁것이라고': 71,\n",
       " '명이': 72,\n",
       " '▁\"': 73,\n",
       " '▁것': 74,\n",
       " '이라고': 75,\n",
       " '▁있다고': 76,\n",
       " '▁것을': 77,\n",
       " 's': 78,\n",
       " '▁4': 79,\n",
       " '나': 80,\n",
       " '’': 81,\n",
       " '▁6': 82,\n",
       " '▁이라크': 83,\n",
       " '시': 84,\n",
       " '▁그러나': 85,\n",
       " '리': 86,\n",
       " '▁5': 87,\n",
       " '게': 88,\n",
       " '▁더': 89,\n",
       " '▁다른': 90,\n",
       " '히': 91,\n",
       " '들이': 92,\n",
       " '▁대통령': 93,\n",
       " '하게': 94,\n",
       " '▁중국': 95,\n",
       " '운': 96,\n",
       " '▁한편': 97,\n",
       " '하기': 98,\n",
       " '라': 99,\n",
       " '▁10': 100,\n",
       " '라고': 101,\n",
       " '▁북한': 102,\n",
       " '하지': 103,\n",
       " '던': 104,\n",
       " '명의': 105,\n",
       " '▁후': 106,\n",
       " '▁주': 107,\n",
       " '적인': 108,\n",
       " '대': 109,\n",
       " '▁이후': 110,\n",
       " '주': 111,\n",
       " '어': 112,\n",
       " '▁통해': 113,\n",
       " '사': 114,\n",
       " '-': 115,\n",
       " '▁정부': 116,\n",
       " '▁많은': 117,\n",
       " '들은': 118,\n",
       " '▁또': 119,\n",
       " '▁위한': 120,\n",
       " '▁대통령은': 121,\n",
       " '서': 122,\n",
       " '▁두': 123,\n",
       " '▁부시': 124,\n",
       " '▁영국': 125,\n",
       " '명': 126,\n",
       " '▁8': 127,\n",
       " '▁가장': 128,\n",
       " '▁동안': 129,\n",
       " '드': 130,\n",
       " '▁내': 131,\n",
       " '▁같은': 132,\n",
       " '▁것이다': 133,\n",
       " '▁7': 134,\n",
       " '▁대변인은': 135,\n",
       " '▁바': 136,\n",
       " '세': 137,\n",
       " '▁한국': 138,\n",
       " '트': 139,\n",
       " '▁등': 140,\n",
       " '까지': 141,\n",
       " '될': 142,\n",
       " '▁주장했다': 143,\n",
       " '이다': 144,\n",
       " '▁알': 145,\n",
       " '수': 146,\n",
       " '▁될': 147,\n",
       " '▁약': 148,\n",
       " '▁것은': 149,\n",
       " '▁한다': 150,\n",
       " '▁세계': 151,\n",
       " '▁할': 152,\n",
       " '▁또한': 153,\n",
       " '▁때': 154,\n",
       " '아': 155,\n",
       " '개': 156,\n",
       " '▁현재': 157,\n",
       " '비': 158,\n",
       " '됐다': 159,\n",
       " '했다고': 160,\n",
       " '적': 161,\n",
       " '르': 162,\n",
       " '▁12': 163,\n",
       " '▁경우': 164,\n",
       " '▁보도했다': 165,\n",
       " '▁이상': 166,\n",
       " '▁덧붙였다': 167,\n",
       " '▁관련': 168,\n",
       " '▁있었다': 169,\n",
       " '▁않았다': 170,\n",
       " '및': 171,\n",
       " '▁9': 172,\n",
       " '성': 173,\n",
       " '▁문제': 174,\n",
       " '▁최근': 175,\n",
       " '▁오바마': 176,\n",
       " '▁했다': 177,\n",
       " '했으며': 178,\n",
       " '하며': 179,\n",
       " '▁영화': 180,\n",
       " '야': 181,\n",
       " '▁일': 182,\n",
       " 'e': 183,\n",
       " '니': 184,\n",
       " '했던': 185,\n",
       " '에는': 186,\n",
       " '면': 187,\n",
       " '들을': 188,\n",
       " '▁자신의': 189,\n",
       " '부': 190,\n",
       " '▁뒤': 191,\n",
       " \"'\": 192,\n",
       " '▁함께': 193,\n",
       " '▁것이': 194,\n",
       " '▁11': 195,\n",
       " '▁일부': 196,\n",
       " '▁새로': 197,\n",
       " \"▁'\": 198,\n",
       " '미': 199,\n",
       " '크': 200,\n",
       " '▁당시': 201,\n",
       " '▁경제': 202,\n",
       " '▁20': 203,\n",
       " '상': 204,\n",
       " '▁모든': 205,\n",
       " '▁그리고': 206,\n",
       " '▁일본': 207,\n",
       " '진': 208,\n",
       " '▁때문에': 209,\n",
       " '전': 210,\n",
       " '소': 211,\n",
       " '▁말': 212,\n",
       " '▁예정이다': 213,\n",
       " '▁수도': 214,\n",
       " '▁19': 215,\n",
       " '▁없다': 216,\n",
       " '치': 217,\n",
       " '▁조사': 218,\n",
       " '▁있습니다': 219,\n",
       " '▁설명했다': 220,\n",
       " '▁지': 221,\n",
       " '▁제': 222,\n",
       " '▁몇': 223,\n",
       " '선': 224,\n",
       " '▁있으며': 225,\n",
       " '해야': 226,\n",
       " '보': 227,\n",
       " '▁경찰은': 228,\n",
       " '원': 229,\n",
       " '▁비': 230,\n",
       " '▁모두': 231,\n",
       " '▁아': 232,\n",
       " '차': 233,\n",
       " '▁미군': 234,\n",
       " '마': 235,\n",
       " '▁정부는': 236,\n",
       " '▁파키스탄': 237,\n",
       " '에서는': 238,\n",
       " '한다': 239,\n",
       " '▁프랑스': 240,\n",
       " '▁유엔': 241,\n",
       " '분': 242,\n",
       " '보다': 243,\n",
       " '억': 244,\n",
       " '▁국가': 245,\n",
       " '▁계속': 246,\n",
       " '당': 247,\n",
       " '약': 248,\n",
       " '▁큰': 249,\n",
       " '▁공격': 250,\n",
       " '파': 251,\n",
       " '▁마': 252,\n",
       " '▁30': 253,\n",
       " '하다': 254,\n",
       " '▁시': 255,\n",
       " '▁사': 256,\n",
       " '▁하는': 257,\n",
       " '화': 258,\n",
       " '▁계획': 259,\n",
       " '▁이들': 260,\n",
       " '▁러시아': 261,\n",
       " '▁지난해': 262,\n",
       " '▁이스라엘': 263,\n",
       " '장': 264,\n",
       " '적으로': 265,\n",
       " '타': 266,\n",
       " '▁경찰': 267,\n",
       " '▁민주당': 268,\n",
       " '▁사고': 269,\n",
       " '▁(': 270,\n",
       " '명을': 271,\n",
       " '▁열린': 272,\n",
       " '▁나': 273,\n",
       " '▁오': 274,\n",
       " '▁대통령이': 275,\n",
       " '▁않을': 276,\n",
       " '▁지역': 277,\n",
       " '들': 278,\n",
       " '▁올해': 279,\n",
       " '여': 280,\n",
       " '제': 281,\n",
       " '용': 282,\n",
       " '오': 283,\n",
       " '하': 284,\n",
       " '2': 285,\n",
       " '▁이날': 286,\n",
       " '프': 287,\n",
       " '▁총리는': 288,\n",
       " '▁우리': 289,\n",
       " '그': 290,\n",
       " '▁the': 291,\n",
       " '바': 292,\n",
       " '▁반': 293,\n",
       " '▁발표했다': 294,\n",
       " '▁경기': 295,\n",
       " '▁핵': 296,\n",
       " '▁클린턴': 297,\n",
       " '▁발생한': 298,\n",
       " '즈': 299,\n",
       " '▁지원': 300,\n",
       " '정': 301,\n",
       " '▁15': 302,\n",
       " '했습니다': 303,\n",
       " '신': 304,\n",
       " '▁받고': 305,\n",
       " '티': 306,\n",
       " '▁후보': 307,\n",
       " 'd': 308,\n",
       " '▁테러': 309,\n",
       " '라는': 310,\n",
       " '조': 311,\n",
       " '간': 312,\n",
       " '에따르면': 313,\n",
       " '▁부': 314,\n",
       " '▁하고': 315,\n",
       " '▁해': 316,\n",
       " '▁결과': 317,\n",
       " '▁있을': 318,\n",
       " '▁거': 319,\n",
       " '디': 320,\n",
       " '었다': 321,\n",
       " '▁가운데': 322,\n",
       " '우': 323,\n",
       " '레': 324,\n",
       " '▁미국의': 325,\n",
       " '위': 326,\n",
       " '▁달러': 327,\n",
       " '▁안': 328,\n",
       " '▁존': 329,\n",
       " '▁16': 330,\n",
       " '시간': 331,\n",
       " '▁데': 332,\n",
       " '▁to': 333,\n",
       " '내': 334,\n",
       " '이나': 335,\n",
       " 'm': 336,\n",
       " '▁달': 337,\n",
       " '▁군': 338,\n",
       " '▁의해': 339,\n",
       " '▁여성': 340,\n",
       " '▁이란': 341,\n",
       " '▁다': 342,\n",
       " '▁된다': 343,\n",
       " '▁선거': 344,\n",
       " '▁국제': 345,\n",
       " '부터': 346,\n",
       " '▁회담': 347,\n",
       " '▁대': 348,\n",
       " '▁사건': 349,\n",
       " '으며': 350,\n",
       " '▁주요': 351,\n",
       " '▁사람들이': 352,\n",
       " '▁정부가': 353,\n",
       " '카': 354,\n",
       " '▁에': 355,\n",
       " '1': 356,\n",
       " '▁18': 357,\n",
       " '▁다시': 358,\n",
       " '▁연구': 359,\n",
       " ')’': 360,\n",
       " '면서': 361,\n",
       " '▁인해': 362,\n",
       " '▁보고': 363,\n",
       " '▁관계': 364,\n",
       " '▁됐다': 365,\n",
       " '산': 366,\n",
       " '에도': 367,\n",
       " '▁세': 368,\n",
       " '입니다': 369,\n",
       " '▁발표': 370,\n",
       " '▁않은': 371,\n",
       " '▁총리': 372,\n",
       " '길': 373,\n",
       " '▁of': 374,\n",
       " '토': 375,\n",
       " '▁초': 376,\n",
       " '질': 377,\n",
       " '러': 378,\n",
       " '▁보': 379,\n",
       " '.”': 380,\n",
       " '▁13': 381,\n",
       " '노': 382,\n",
       " '▁첫': 383,\n",
       " '▁혐의로': 384,\n",
       " '▁결정': 385,\n",
       " '▁시간': 386,\n",
       " '하면서': 387,\n",
       " '▁무': 388,\n",
       " 'S': 389,\n",
       " '▁총': 390,\n",
       " '려': 391,\n",
       " '▁있다는': 392,\n",
       " '계': 393,\n",
       " '▁채': 394,\n",
       " '▁정': 395,\n",
       " '되는': 396,\n",
       " 'y': 397,\n",
       " '코': 398,\n",
       " '모': 399,\n",
       " '린': 400,\n",
       " '▁조': 401,\n",
       " '▁노': 402,\n",
       " '▁불': 403,\n",
       " '▁조치': 404,\n",
       " '되어': 405,\n",
       " '▁이미': 406,\n",
       " '▁프로그램': 407,\n",
       " '▁시위': 408,\n",
       " '들의': 409,\n",
       " '▁14': 410,\n",
       " '▁위치한': 411,\n",
       " '▁받았다': 412,\n",
       " 'km': 413,\n",
       " '▁기': 414,\n",
       " '구': 415,\n",
       " '▁우려': 416,\n",
       " '▁강': 417,\n",
       " '▁팔레스타인': 418,\n",
       " '국': 419,\n",
       " '호': 420,\n",
       " '▁아프간': 421,\n",
       " '▁줄': 422,\n",
       " '번': 423,\n",
       " '브': 424,\n",
       " '▁인도': 425,\n",
       " 'a': 426,\n",
       " '▁없는': 427,\n",
       " '식': 428,\n",
       " '지만': 429,\n",
       " '▁방문': 430,\n",
       " '돼': 431,\n",
       " '▁성': 432,\n",
       " '▁승리': 433,\n",
       " '권': 434,\n",
       " '하여': 435,\n",
       " '▁차': 436,\n",
       " '▁아직': 437,\n",
       " '▁TV': 438,\n",
       " '▁유': 439,\n",
       " '▁감독': 440,\n",
       " '▁피해': 441,\n",
       " '▁파': 442,\n",
       " '▁모': 443,\n",
       " '이라며': 444,\n",
       " '▁추가': 445,\n",
       " '▁사실을': 446,\n",
       " '로부터': 447,\n",
       " '▁25': 448,\n",
       " '▁하': 449,\n",
       " '단': 450,\n",
       " '▁혐의': 451,\n",
       " '▁관계자는': 452,\n",
       " '▁뉴욕': 453,\n",
       " '▁않고': 454,\n",
       " '▁간': 455,\n",
       " '관': 456,\n",
       " '▁유럽': 457,\n",
       " '루': 458,\n",
       " '/': 459,\n",
       " '▁독일': 460,\n",
       " '▁회사': 461,\n",
       " '▁한다고': 462,\n",
       " '▁지적했다': 463,\n",
       " \"▁''\": 464,\n",
       " '더': 465,\n",
       " '▁최고': 466,\n",
       " '▁대선': 467,\n",
       " '▁지지': 468,\n",
       " '▁17': 469,\n",
       " '▁피': 470,\n",
       " '습니다': 471,\n",
       " '▁지금': 472,\n",
       " '달러': 473,\n",
       " '▁W': 474,\n",
       " '되고': 475,\n",
       " '▁잘': 476,\n",
       " '▁포함': 477,\n",
       " '▁기록': 478,\n",
       " '▁구': 479,\n",
       " '▁활동': 480,\n",
       " '▁아이': 481,\n",
       " 'A': 482,\n",
       " '▁명의': 483,\n",
       " '▁신': 484,\n",
       " '▁협상': 485,\n",
       " '년간': 486,\n",
       " '▁조지': 487,\n",
       " '▁매우': 488,\n",
       " '▁자': 489,\n",
       " '▁소속': 490,\n",
       " '▁카': 491,\n",
       " '▁원': 492,\n",
       " '▁재': 493,\n",
       " '▁내용': 494,\n",
       " '▁시장': 495,\n",
       " '▁올림픽': 496,\n",
       " '살': 497,\n",
       " '▁이슬람': 498,\n",
       " '▁당국은': 499,\n",
       " '물': 500,\n",
       " '실': 501,\n",
       " '▁관리': 502,\n",
       " '▁탈레반': 503,\n",
       " '▁대부분': 504,\n",
       " '직': 505,\n",
       " '난': 506,\n",
       " '▁매케인': 507,\n",
       " '든': 508,\n",
       " '중': 509,\n",
       " '동': 510,\n",
       " '▁in': 511,\n",
       " '▁호주': 512,\n",
       " '개월': 513,\n",
       " 'ing': 514,\n",
       " '▁없다고': 515,\n",
       " '▁차량': 516,\n",
       " '▁대학': 517,\n",
       " '▁타': 518,\n",
       " '법': 519,\n",
       " '▁이는': 520,\n",
       " '▁받은': 521,\n",
       " '였다': 522,\n",
       " '▁예정': 523,\n",
       " '▁못했다': 524,\n",
       " '체': 525,\n",
       " '처럼': 526,\n",
       " '▁않는다': 527,\n",
       " '점': 528,\n",
       " '▁상원의원': 529,\n",
       " '▁인터넷': 530,\n",
       " '안': 531,\n",
       " '회': 532,\n",
       " '▁사람': 533,\n",
       " '▁이에': 534,\n",
       " '3': 535,\n",
       " 'ed': 536,\n",
       " '▁비난': 537,\n",
       " '공': 538,\n",
       " '되지': 539,\n",
       " '?': 540,\n",
       " '▁2006': 541,\n",
       " '▁많': 542,\n",
       " '5': 543,\n",
       " '▁어떤': 544,\n",
       " '▁24': 545,\n",
       " '▁주장': 546,\n",
       " '▁오늘': 547,\n",
       " '▁부상': 548,\n",
       " 'o': 549,\n",
       " '▁시작': 550,\n",
       " '▁여러': 551,\n",
       " '▁하지': 552,\n",
       " '▁판매': 553,\n",
       " '력': 554,\n",
       " '▁새': 555,\n",
       " '▁하지만': 556,\n",
       " '▁폭탄': 557,\n",
       " '▁사망': 558,\n",
       " '▁100': 559,\n",
       " '▁있던': 560,\n",
       " '▁공': 561,\n",
       " '▁물': 562,\n",
       " '▁오후': 563,\n",
       " '▁21': 564,\n",
       " '▁석방': 565,\n",
       " '무': 566,\n",
       " '▁50': 567,\n",
       " '유': 568,\n",
       " '▁입장': 569,\n",
       " '발': 570,\n",
       " '▁생각': 571,\n",
       " '▁A': 572,\n",
       " '▁전쟁': 573,\n",
       " '▁당': 574,\n",
       " '명은': 575,\n",
       " '▁매': 576,\n",
       " '▁양': 577,\n",
       " '▁검찰': 578,\n",
       " '재': 579,\n",
       " '▁방송': 580,\n",
       " '▁상': 581,\n",
       " '종': 582,\n",
       " '경': 583,\n",
       " '▁도': 584,\n",
       " '▁날': 585,\n",
       " '터': 586,\n",
       " '▁미얀마': 587,\n",
       " '▁27': 588,\n",
       " '▁기자': 589,\n",
       " '형': 590,\n",
       " '되었다': 591,\n",
       " '릴': 592,\n",
       " '반': 593,\n",
       " '▁배': 594,\n",
       " '임': 595,\n",
       " '들에게': 596,\n",
       " '▁정책': 597,\n",
       " '피': 598,\n",
       " '▁있어': 599,\n",
       " '하는데': 600,\n",
       " '▁해결': 601,\n",
       " '이며': 602,\n",
       " '▁리': 603,\n",
       " '데': 604,\n",
       " '▁자동차': 605,\n",
       " '개의': 606,\n",
       " '▁23': 607,\n",
       " '▁된': 608,\n",
       " '▁반대': 609,\n",
       " '▁인터뷰에서': 610,\n",
       " '▁준비': 611,\n",
       " '▁소': 612,\n",
       " '이었다': 613,\n",
       " '▁병원': 614,\n",
       " '▁22': 615,\n",
       " '▁우주': 616,\n",
       " '▁라': 617,\n",
       " '▁김': 618,\n",
       " '▁컴퓨터': 619,\n",
       " '▁발': 620,\n",
       " '▁강조했다': 621,\n",
       " '▁것에': 622,\n",
       " '▁이러': 623,\n",
       " '했지만': 624,\n",
       " '▁갖고': 625,\n",
       " '했고': 626,\n",
       " '▁노력': 627,\n",
       " '▁스': 628,\n",
       " '포': 629,\n",
       " '▁그들은': 630,\n",
       " '▁개발': 631,\n",
       " '감': 632,\n",
       " '▁기술': 633,\n",
       " '▁공화당': 634,\n",
       " '▁점': 635,\n",
       " '▁도시': 636,\n",
       " 'i': 637,\n",
       " '▁합의': 638,\n",
       " '들과': 639,\n",
       " '▁S': 640,\n",
       " '▁개': 641,\n",
       " '▁앞서': 642,\n",
       " '▁산': 643,\n",
       " '▁떨어진': 644,\n",
       " '석': 645,\n",
       " '▁원문기사보기': 646,\n",
       " '▁있도록': 647,\n",
       " '▁오전': 648,\n",
       " '▁대표': 649,\n",
       " '▁서': 650,\n",
       " '▁사이': 651,\n",
       " '▁발생': 652,\n",
       " '▁마지막': 653,\n",
       " '▁남부': 654,\n",
       " 't': 655,\n",
       " '▁비난했다': 656,\n",
       " '▁투표': 657,\n",
       " '▁이어': 658,\n",
       " '▁북한이': 659,\n",
       " ':': 660,\n",
       " '▁터키': 661,\n",
       " '키': 662,\n",
       " '온': 663,\n",
       " '▁안전': 664,\n",
       " '▁가진': 665,\n",
       " '▁폭발': 666,\n",
       " '▁경찰이': 667,\n",
       " '▁증가': 668,\n",
       " '연': 669,\n",
       " '▁오는': 670,\n",
       " '▁변화': 671,\n",
       " '에따라': 672,\n",
       " '하기로': 673,\n",
       " '▁2005': 674,\n",
       " '심': 675,\n",
       " '했으나': 676,\n",
       " '▁미국과': 677,\n",
       " '▁의사': 678,\n",
       " '▁볼': 679,\n",
       " '네': 680,\n",
       " '7': 681,\n",
       " '▁장관은': 682,\n",
       " 'c': 683,\n",
       " '▁이를': 684,\n",
       " '▁인근': 685,\n",
       " '▁외': 686,\n",
       " '▁자신이': 687,\n",
       " '▁처음으로': 688,\n",
       " '▁상황': 689,\n",
       " '▁그녀': 690,\n",
       " '방': 691,\n",
       " '버': 692,\n",
       " '▁처음': 693,\n",
       " '거': 694,\n",
       " 'al': 695,\n",
       " '▁그녀는': 696,\n",
       " 'C': 697,\n",
       " '이라는': 698,\n",
       " '문': 699,\n",
       " '▁위협': 700,\n",
       " '▁규모': 701,\n",
       " '했었다': 702,\n",
       " '▁사용': 703,\n",
       " '▁생산': 704,\n",
       " '▁남자': 705,\n",
       " '▁버락': 706,\n",
       " '▁성명': 707,\n",
       " '으로부터': 708,\n",
       " '▁지난달': 709,\n",
       " '▁반군': 710,\n",
       " '군': 711,\n",
       " '▁아시아': 712,\n",
       " '▁치료': 713,\n",
       " 'P': 714,\n",
       " '▁없었다': 715,\n",
       " '▁여행': 716,\n",
       " '▁26': 717,\n",
       " '▁선': 718,\n",
       " '▁힐러리': 719,\n",
       " '▁이탈리아': 720,\n",
       " '학': 721,\n",
       " '▁40': 722,\n",
       " '▁최소': 723,\n",
       " '▁지역에서': 724,\n",
       " '▁코': 725,\n",
       " '▁금융': 726,\n",
       " '▁정치': 727,\n",
       " '거나': 728,\n",
       " '▁연방': 729,\n",
       " '▁앞으로': 730,\n",
       " '▁CNN': 731,\n",
       " '▁있지만': 732,\n",
       " '▁a': 733,\n",
       " '▁영향': 734,\n",
       " '하도록': 735,\n",
       " '▁야당': 736,\n",
       " '▁열': 737,\n",
       " '▁없이': 738,\n",
       " '6': 739,\n",
       " '▁보인다': 740,\n",
       " '▁보도': 741,\n",
       " '▁관한': 742,\n",
       " '위원회': 743,\n",
       " '▁̋': 744,\n",
       " '▁사망했다': 745,\n",
       " '지는': 746,\n",
       " '▁장': 747,\n",
       " '▁다음': 748,\n",
       " '▁불법': 749,\n",
       " '▁실': 750,\n",
       " '와의': 751,\n",
       " 'n': 752,\n",
       " '▁이용': 753,\n",
       " '▁스페인': 754,\n",
       " '교': 755,\n",
       " '요': 756,\n",
       " '▁중단': 757,\n",
       " '▁않다': 758,\n",
       " '▁사망자': 759,\n",
       " '▁만에': 760,\n",
       " '▁좋은': 761,\n",
       " '▁의회': 762,\n",
       " '▁살해': 763,\n",
       " '됐다고': 764,\n",
       " '▁미국이': 765,\n",
       " '강': 766,\n",
       " '배': 767,\n",
       " '▁29': 768,\n",
       " '▁사람들': 769,\n",
       " '▁이름': 770,\n",
       " '츠': 771,\n",
       " '스트': 772,\n",
       " '▁현지': 773,\n",
       " 'er': 774,\n",
       " '▁가격': 775,\n",
       " '번째': 776,\n",
       " '_': 777,\n",
       " 'E': 778,\n",
       " '된다': 779,\n",
       " '▁당국': 780,\n",
       " '▁발견': 781,\n",
       " '▁상태': 782,\n",
       " '▁은행': 783,\n",
       " '▁공식': 784,\n",
       " '우리는': 785,\n",
       " '란': 786,\n",
       " '▁사태': 787,\n",
       " '▁런던': 788,\n",
       " 'N': 789,\n",
       " '▁행사': 790,\n",
       " '▁언론': 791,\n",
       " '▁미사일': 792,\n",
       " '▁가능성': 793,\n",
       " '▁팀': 794,\n",
       " '는데': 795,\n",
       " '▁밤': 796,\n",
       " '▁사람들은': 797,\n",
       " '▁평화': 798,\n",
       " '▁28': 799,\n",
       " '▁거래': 800,\n",
       " '▁어': 801,\n",
       " '건': 802,\n",
       " '론': 803,\n",
       " '▁무기': 804,\n",
       " 'article': 805,\n",
       " '▁백악관': 806,\n",
       " '▁적': 807,\n",
       " '▁이들은': 808,\n",
       " '▁주둔': 809,\n",
       " '▁알려졌다': 810,\n",
       " 'M': 811,\n",
       " '날': 812,\n",
       " '▁자신': 813,\n",
       " '천': 814,\n",
       " '▁AP': 815,\n",
       " '▁논의': 816,\n",
       " '▁벌이': 817,\n",
       " '▁심': 818,\n",
       " '▁불구하고': 819,\n",
       " '▁중요한': 820,\n",
       " '▁연기': 821,\n",
       " '▁오바마는': 822,\n",
       " '됐으며': 823,\n",
       " 'u': 824,\n",
       " '▁집': 825,\n",
       " '▁발언': 826,\n",
       " '▁말했습니다': 827,\n",
       " '▁바그다드': 828,\n",
       " '▁이전': 829,\n",
       " '▁발생했다': 830,\n",
       " '▁박사는': 831,\n",
       " 'p': 832,\n",
       " '▁명': 833,\n",
       " '▁대통령과': 834,\n",
       " '▁사망한': 835,\n",
       " '▁게임': 836,\n",
       " '두': 837,\n",
       " '▁필요': 838,\n",
       " '병': 839,\n",
       " '행': 840,\n",
       " '▁우': 841,\n",
       " '▁것입니다': 842,\n",
       " '청': 843,\n",
       " '▁지구': 844,\n",
       " '▁자금': 845,\n",
       " '▁용의자': 846,\n",
       " '▁B': 847,\n",
       " '▁이런': 848,\n",
       " '급': 849,\n",
       " '▁정도': 850,\n",
       " '▁미국은': 851,\n",
       " '▁역할': 852,\n",
       " '▁반면': 853,\n",
       " '▁돈': 854,\n",
       " '▁조직': 855,\n",
       " '너': 856,\n",
       " 'I': 857,\n",
       " '▁관심': 858,\n",
       " '▁올': 859,\n",
       " '▁법원': 860,\n",
       " '▁코리아': 861,\n",
       " '▁2008': 862,\n",
       " '▁비행기': 863,\n",
       " '▁공동': 864,\n",
       " '▁투자': 865,\n",
       " '▁인질': 866,\n",
       " '▁역사': 867,\n",
       " '▁이야기': 868,\n",
       " '▁상승': 869,\n",
       " 'D': 870,\n",
       " '▁학교': 871,\n",
       " '’’': 872,\n",
       " 'and': 873,\n",
       " '▁공급': 874,\n",
       " 'T': 875,\n",
       " '▁최대': 876,\n",
       " '▁법': 877,\n",
       " '▁동': 878,\n",
       " '▁포': 879,\n",
       " '▁무샤라프': 880,\n",
       " '저': 881,\n",
       " '▁보안': 882,\n",
       " '▁북부': 883,\n",
       " '▁고위': 884,\n",
       " '▁보다': 885,\n",
       " '울': 886,\n",
       " '▁회장': 887,\n",
       " '▁사망했다고': 888,\n",
       " '장은': 889,\n",
       " '▁치': 890,\n",
       " '▁서비스': 891,\n",
       " '2005.08': 892,\n",
       " '▁방': 893,\n",
       " '▁메': 894,\n",
       " 'F': 895,\n",
       " '▁살': 896,\n",
       " '▁위험': 897,\n",
       " '마리': 898,\n",
       " '▁정치적': 899,\n",
       " '설': 900,\n",
       " '래': 901,\n",
       " '워': 902,\n",
       " '▁의하면': 903,\n",
       " '▁단체': 904,\n",
       " '▁입': 905,\n",
       " '▁알카에다': 906,\n",
       " '▁자신들': 907,\n",
       " '▁앞': 908,\n",
       " '▁모습을': 909,\n",
       " '▁2005.08': 910,\n",
       " '금': 911,\n",
       " '▁건물': 912,\n",
       " '포인트': 913,\n",
       " 'CNN': 914,\n",
       " '▁워싱턴': 915,\n",
       " '▁의원': 916,\n",
       " '▁지도자': 917,\n",
       " '▁부족': 918,\n",
       " '▁충돌': 919,\n",
       " '▁수사': 920,\n",
       " '▁발사': 921,\n",
       " '▁위': 922,\n",
       " '▁군사': 923,\n",
       " '시킬': 924,\n",
       " '▁것이라는': 925,\n",
       " '▁건강': 926,\n",
       " '▁자살': 927,\n",
       " '▁요구': 928,\n",
       " '친': 929,\n",
       " '▁행위': 930,\n",
       " '하면': 931,\n",
       " '▁공항': 932,\n",
       " '▁우승': 933,\n",
       " '▁출신': 934,\n",
       " '▁감소': 935,\n",
       " '▁여': 936,\n",
       " '▁가능성이': 937,\n",
       " '▁높은': 938,\n",
       " '▁더욱': 939,\n",
       " '▁지난주': 940,\n",
       " '▁왔다': 941,\n",
       " '기를': 942,\n",
       " '▁뜻': 943,\n",
       " '류': 944,\n",
       " '4': 945,\n",
       " '▁국경': 946,\n",
       " '▁인용': 947,\n",
       " '▁행동': 948,\n",
       " '팀': 949,\n",
       " '▁전에': 950,\n",
       " '후': 951,\n",
       " 'r': 952,\n",
       " '’(': 953,\n",
       " '▁막': 954,\n",
       " '슨': 955,\n",
       " '▁성명에서': 956,\n",
       " '▁뉴스': 957,\n",
       " '▁서울': 958,\n",
       " '작': 959,\n",
       " '▁하마스': 960,\n",
       " '하자': 961,\n",
       " '▁&': 962,\n",
       " '▁지역에': 963,\n",
       " '세의': 964,\n",
       " '간의': 965,\n",
       " '▁말을': 966,\n",
       " '▁보호': 967,\n",
       " '만달러': 968,\n",
       " '▁N': 969,\n",
       " '판': 970,\n",
       " '▁힘': 971,\n",
       " '▁하나': 972,\n",
       " '▁가족': 973,\n",
       " '▁이와': 974,\n",
       " '한다고': 975,\n",
       " '속': 976,\n",
       " '▁아니다': 977,\n",
       " 'B': 978,\n",
       " '▁것이며': 979,\n",
       " '▁인쇄': 980,\n",
       " '▁레': 981,\n",
       " '▁검사': 982,\n",
       " '▁알고': 983,\n",
       " '▁2004': 984,\n",
       " '이번': 985,\n",
       " '▁총리가': 986,\n",
       " '초': 987,\n",
       " '▁for': 988,\n",
       " '▁호': 989,\n",
       " '▁않는': 990,\n",
       " '▁분': 991,\n",
       " '▁길': 992,\n",
       " '▁독립': 993,\n",
       " '▁부토': 994,\n",
       " 'ar': 995,\n",
       " '▁전화': 996,\n",
       " '민': 997,\n",
       " '였던': 998,\n",
       " '양': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "511ca07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁아', '▁더', '빙', '.', '.', '▁진짜', '▁짜', '증', '나', '네', '요', '▁목소리', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#print(word_index)\n",
    "#print(index_word)\n",
    "\n",
    "for sequence in tensor:\n",
    "    print([index_word[word]for word in sequence])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7833c2",
   "metadata": {},
   "source": [
    "LSTM 기반의 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8cf41e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:03<00:00, 41934.70it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:01<00:00, 45488.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Mecab : 한국어 형태소 분석기\n",
    "#tokenizer = Mecab()\n",
    "\n",
    "def load_data(train_data, test_data, s,num_words=10000):\n",
    "    # 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document'])\n",
    "    test_data = test_data.drop_duplicates(subset=['document'])\n",
    "\n",
    "    # Nan 결측치 제거\n",
    "    # https://wikidocs.net/153202\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    # Use SentencePiece\n",
    "    train_tokens, word_to_index, _ = sp_tokenize(s, train_data['document'], maxlen=40)\n",
    "    test_tokens, _, _ = sp_tokenize(s, test_data['document'],maxlen=40)\n",
    "    \n",
    "    return train_tokens, np.array(train_data['label']), test_tokens, test_data['label'], word_to_index\n",
    "\n",
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt')\n",
    "\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "407aa66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126162, 40)\n",
      "(126162,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81bc1988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126162, 40)\n",
      "(126162,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79236ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "662699fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 4s 7ms/step - loss: 0.5707 - accuracy: 0.6792 - val_loss: 0.4178 - val_accuracy: 0.8178\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3916 - accuracy: 0.8302 - val_loss: 0.3908 - val_accuracy: 0.8254\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3677 - accuracy: 0.8410 - val_loss: 0.3857 - val_accuracy: 0.8259\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3572 - accuracy: 0.8463 - val_loss: 0.3926 - val_accuracy: 0.8221\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3500 - accuracy: 0.8500 - val_loss: 0.3883 - val_accuracy: 0.8270\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a271eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/1536 - 3s - loss: 0.3940 - accuracy: 0.8245\n",
      "[0.3939913809299469, 0.824516773223877]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47409e84",
   "metadata": {},
   "source": [
    "# 영화 데이터로 토크나이저 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6200e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(data_path)\n",
    "train_data = train_data.dropna(subset=['document'])\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "clean_path = \"spm_input.txt\"\n",
    "\n",
    "with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in train_data['document']:\n",
    "        f.write(str(line).strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5e7dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=spm_input.txt --model_prefix=korean_spm --vocab_size=16000  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_input.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: spm_input.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 149995 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430579\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1686\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 149995 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308596 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 149995\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357030\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 357030 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157510 obj=15.4277 num_tokens=841361 num_tokens/piece=5.34164\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145724 obj=14.3586 num_tokens=846835 num_tokens/piece=5.81123\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109233 obj=14.4548 num_tokens=882026 num_tokens/piece=8.07472\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109051 obj=14.3996 num_tokens=882760 num_tokens/piece=8.09493\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81784 obj=14.6341 num_tokens=926726 num_tokens/piece=11.3314\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81776 obj=14.5731 num_tokens=926844 num_tokens/piece=11.3339\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61332 obj=14.8307 num_tokens=968603 num_tokens/piece=15.7928\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61332 obj=14.7702 num_tokens=968697 num_tokens/piece=15.7943\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45999 obj=15.0587 num_tokens=1013505 num_tokens/piece=22.0332\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45999 obj=14.9947 num_tokens=1013498 num_tokens/piece=22.033\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34499 obj=15.3164 num_tokens=1059628 num_tokens/piece=30.7147\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34499 obj=15.2503 num_tokens=1059642 num_tokens/piece=30.7152\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25874 obj=15.606 num_tokens=1107401 num_tokens/piece=42.7998\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25874 obj=15.5339 num_tokens=1107420 num_tokens/piece=42.8005\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19405 obj=15.918 num_tokens=1157690 num_tokens/piece=59.6594\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19405 obj=15.838 num_tokens=1157711 num_tokens/piece=59.6604\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=15.9719 num_tokens=1175309 num_tokens/piece=66.7789\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=15.9439 num_tokens=1175315 num_tokens/piece=66.7793\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25689/3447544542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spm.SentencePieceTrainer.Train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'--input={} --model_prefix=korean_spm --vocab_size={}  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"korean_spm.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m       \u001b[0;34m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_TrainFromString\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_TrainFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(clean_path, vocab_size)    \n",
    ")\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.load(\"korean_spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, word_index, index_word = sp_tokenize(s, train_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_index)\n",
    "#print(index_word)\n",
    "\n",
    "for sequence in tensor:\n",
    "    print([index_word[word]for word in sequence])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt')\n",
    "\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s)\n",
    "\n",
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0871948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4336e",
   "metadata": {},
   "source": [
    "# KoNLPy 형태소 분석기 사용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab : 한국어 형태소 분석기\n",
    "tokenizer = Mecab()\n",
    "maxlen = 40\n",
    "\n",
    "def load_data(train_data, test_data, s,num_words=10000):\n",
    "    # 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document'])\n",
    "    test_data = test_data.drop_duplicates(subset=['document'])\n",
    "\n",
    "    # Nan 결측치 제거\n",
    "    # https://wikidocs.net/153202\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    # 토큰화, 형태소에 대한 구문 분석\n",
    "    # https://konlpy.org/ko/v0.6.0/api/konlpy.tag/\n",
    "    train_tokens = [tokenizer.morphs(sentence) for sentence in train_data['document']]\n",
    "    test_tokens =  [tokenizer.morphs(sentence) for sentence in test_data['document']]\n",
    "\n",
    "    # Use SentencePiece\n",
    "    #train_tokens, word_to_index, _ = sp_tokenize(s, train_data['document'])\n",
    "    #test_tokens, _, _ = sp_tokenize(s, test_data['document'])\n",
    "    \n",
    "    # word_to_index 구성\n",
    "    # Counter를 사용하여 많이 사용된 num_words 개의 단어 vocab 만들기\n",
    "    word_to_index = {}\n",
    "    # 2-d list를 1-d list로 변환\n",
    "    words = np.concatenate(train_tokens).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(num_words-4)\n",
    "    vocab = [key for key, _ in counter]\n",
    "    # 미리 정의된 토큰 4개를 제외하고 단어 인덱스 부여\n",
    "    word_to_index = {word:index+4 for index, word in enumerate(vocab)}\n",
    "\n",
    "    word_to_index['<pad>']=  0# 패딩용 단어\n",
    "    word_to_index['<s>']=  1# 문장의 시작지점\n",
    "    word_to_index['</s>']= 2\n",
    "    word_to_index['<unk>']= 3\n",
    "    \n",
    "    # text string to vocab index string\n",
    "    # X_train의 상위 10000개의 단어만 단어에 등록되었기 때문에, X_train에도 <UNK>토큰이 발생할 수 있다.\n",
    "    X_train = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in train_token] for train_token in train_tokens]\n",
    "    X_test = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in test_token] for test_token in test_tokens]\n",
    "    \n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "    \n",
    "    return X_train, np.array(train_data['label']), X_test, test_data['label'], word_to_index\n",
    "\n",
    "train_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt')\n",
    "test_data = pd.read_table(os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt')\n",
    "\n",
    "train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "print(np.unique(partial_y_train))\n",
    "print(np.unique(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7731e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63646e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f03b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OOV% : 46.68\n",
      "Test  OOV% : 46.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def oov_ratio(encoded, unk_id=0):\n",
    "    total = sum(len(seq) for seq in encoded)\n",
    "    unk   = sum((tok == unk_id) for seq in encoded for tok in seq)\n",
    "    return round(100 * unk / total, 2)\n",
    "\n",
    "print(\"Train OOV% :\", oov_ratio(X_train))\n",
    "print(\"Test  OOV% :\", oov_ratio(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d07c1b",
   "metadata": {},
   "source": [
    "# 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "001bc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data, test_data, tokenizer, max_len=40, num_words=10000, use_sentence_piece=True):\n",
    "    # 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document'])\n",
    "    test_data = test_data.drop_duplicates(subset=['document'])\n",
    "\n",
    "    # Nan 결측치 제거\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    if use_sentence_piece:\n",
    "        train_tokens, word_to_index, _ = sp_tokenize(tokenizer, train_data['document'], maxlen=max_len)\n",
    "        test_tokens, _, _ = sp_tokenize(tokenizer, test_data['document'], maxlen=max_len)\n",
    "        \n",
    "        return train_tokens, np.array(train_data['label']), test_tokens, test_data['label'], word_to_index\n",
    "    else:\n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '.']\n",
    "\n",
    "        # 토큰화, 형태소에 대한 구문 분석\n",
    "        train_tokens = [tokenizer.morphs(sentence) for sentence in train_data['document']]\n",
    "        test_tokens =  [tokenizer.morphs(sentence) for sentence in test_data['document']]\n",
    "        \n",
    "        # 불용어 제거\n",
    "        train_tokens = [[token for token in sentence if token not in stopwords] for sentence in train_tokens]\n",
    "        test_tokens =  [[token for token in sentence if token not in stopwords] for sentence in test_tokens]\n",
    "\n",
    "        \n",
    "        # word_to_index 구성\n",
    "        word_to_index = {}\n",
    "        words = np.concatenate(train_tokens).tolist()\n",
    "        counter = Counter(words)\n",
    "        counter = counter.most_common(num_words-4)\n",
    "        vocab = [key for key, _ in counter]\n",
    "        word_to_index = {word:index+4 for index, word in enumerate(vocab)}\n",
    "\n",
    "        word_to_index['<pad>']=  0# 패딩용 단어\n",
    "        word_to_index['<s>']=  1# 문장의 시작지점\n",
    "        word_to_index['</s>']= 2\n",
    "        word_to_index['<unk>']= 3\n",
    "    \n",
    "        X_train = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in train_token] for train_token in train_tokens]\n",
    "        X_test = [[word_to_index['<s>']]+[word_to_index[token] if token in word_to_index else word_to_index['<unk>'] for token in test_token] for test_token in test_tokens]\n",
    "\n",
    "        X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "        X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='pre', value=word_to_index[\"<pad>\"], maxlen=maxlen)\n",
    "    \n",
    "        return X_train, np.array(train_data['label']), X_test, test_data['label'], word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aa85cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt\n",
    "\n",
    "def review_model(train_path, test_path, model, tokenizer='sp_ke_park', vocab_size = 16000, val_len=20000, epochs=20):\n",
    "    assert tokenizer in ['sp_ke_park','se', 'mecab', 'hannanum', 'kkma', 'komoran', 'okt']\n",
    "    \n",
    "    train_data = pd.read_table(train_path)\n",
    "    test_data = pd.read_table(test_path)\n",
    "    \n",
    "    train_data['document'] = train_data['document'].astype(str).apply(preprocessing)\n",
    "    test_data['document'] = test_data['document'].astype(str).apply(preprocessing)\n",
    "\n",
    "    if tokenizer == 'sp_ke_park':\n",
    "        temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "        with open(temp_file, 'w') as f:\n",
    "            for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "                f.write(str(row) + '\\n')\n",
    "\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            '--input={} --model_prefix=korean_spm --vocab_size={} --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(temp_file, vocab_size)    \n",
    "        )\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.load(\"korean_spm.model\")\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer)\n",
    "    elif tokenizer == 'se':\n",
    "        clean_path = \"spm_input.txt\"\n",
    "\n",
    "        with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in train_data['document']:\n",
    "                f.write(str(line).strip() + \"\\n\")\n",
    "\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            '--input={} --model_prefix=korean_spm --vocab_size={}  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'.format(clean_path, vocab_size)    \n",
    "        )\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.load(\"korean_spm.model\")\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer)\n",
    "    elif tokenizer == 'mecab':\n",
    "        tokenizer = Mecab()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'hannanum':\n",
    "        tokenizer = Hannanum()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'kkma':\n",
    "        tokenizer = Kkma()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'komoran':\n",
    "        tokenizer = Komoran()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    elif tokenizer == 'okt':\n",
    "        tokenizer = Okt()\n",
    "        X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data, tokenizer, use_sentence_piece=False, num_words=vocab_size)\n",
    "    \n",
    "    \n",
    "    #print(\"Train OOV% :\", oov_ratio(X_train))\n",
    "    #print(\"Test  OOV% :\", oov_ratio(X_test))\n",
    "    \n",
    "    X_val = X_train[:val_len]   \n",
    "    y_val = y_train[:val_len]\n",
    "\n",
    "    partial_x_train = X_train[val_len:]  \n",
    "    partial_y_train = y_train[val_len:]\n",
    "    \n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               patience=2,\n",
    "                               restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(partial_x_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop, checkpoint])\n",
    "    results = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(results)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d278980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1604ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=spm_input.txt --model_prefix=korean_spm --vocab_size=16000  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_input.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: spm_input.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 150000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430599\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1686\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 150000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308597 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 150000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357031\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 357031 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157511 obj=15.4277 num_tokens=841362 num_tokens/piece=5.34161\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145725 obj=14.3586 num_tokens=846836 num_tokens/piece=5.81119\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109233 obj=14.4548 num_tokens=882029 num_tokens/piece=8.07475\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109051 obj=14.3996 num_tokens=882763 num_tokens/piece=8.09496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81784 obj=14.6341 num_tokens=926725 num_tokens/piece=11.3314\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81776 obj=14.5731 num_tokens=926844 num_tokens/piece=11.3339\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61332 obj=14.8307 num_tokens=968598 num_tokens/piece=15.7927\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61332 obj=14.7702 num_tokens=968692 num_tokens/piece=15.7942\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45999 obj=15.0586 num_tokens=1013498 num_tokens/piece=22.033\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45999 obj=14.9947 num_tokens=1013492 num_tokens/piece=22.0329\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34499 obj=15.3164 num_tokens=1059614 num_tokens/piece=30.7143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34499 obj=15.2502 num_tokens=1059627 num_tokens/piece=30.7147\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25874 obj=15.6061 num_tokens=1107398 num_tokens/piece=42.7996\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25874 obj=15.534 num_tokens=1107416 num_tokens/piece=42.8003\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19405 obj=15.9185 num_tokens=1157734 num_tokens/piece=59.6616\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19405 obj=15.8384 num_tokens=1157755 num_tokens/piece=59.6627\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=15.9723 num_tokens=1175362 num_tokens/piece=66.7819\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=15.9443 num_tokens=1175368 num_tokens/piece=66.7823\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n",
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:03<00:00, 46191.42it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:00<00:00, 49299.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OOV% : 59.16\n",
      "Test  OOV% : 59.1\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 3s 8ms/step - loss: 0.5720 - accuracy: 0.6665 - val_loss: 0.3832 - val_accuracy: 0.8387\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3351 - accuracy: 0.8617 - val_loss: 0.3476 - val_accuracy: 0.8508\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.2925 - accuracy: 0.8807 - val_loss: 0.3399 - val_accuracy: 0.8567\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2740 - accuracy: 0.8882 - val_loss: 0.3492 - val_accuracy: 0.8522\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8922 - val_loss: 0.3754 - val_accuracy: 0.8413\n",
      "1536/1536 - 3s - loss: 0.3521 - accuracy: 0.8483\n",
      "[0.3521427512168884, 0.8483214378356934]\n"
     ]
    }
   ],
   "source": [
    "train_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_train.txt'\n",
    "test_path = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc/ratings_test.txt'\n",
    "\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a778461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train OOV% : 62.1\n",
      "Test  OOV% : 62.22\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 3s 7ms/step - loss: 0.5244 - accuracy: 0.7609 - val_loss: 0.3775 - val_accuracy: 0.8414\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3463 - accuracy: 0.8548 - val_loss: 0.3491 - val_accuracy: 0.8496\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.3468 - val_accuracy: 0.8514\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2978 - accuracy: 0.8785 - val_loss: 0.3480 - val_accuracy: 0.8517\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2849 - accuracy: 0.8848 - val_loss: 0.3572 - val_accuracy: 0.8475\n",
      "1536/1536 - 3s - loss: 0.3554 - accuracy: 0.8467\n",
      "[0.35537293553352356, 0.8466938138008118]\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42103b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=16000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=14.4669 num_tokens=682473 num_tokens/piece=38.7769\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=14.4447 num_tokens=682479 num_tokens/piece=38.7772\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n",
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:03<00:00, 41239.61it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:01<00:00, 40862.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OOV% : 46.68\n",
      "Test  OOV% : 46.98\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 3s 7ms/step - loss: 0.5553 - accuracy: 0.7035 - val_loss: 0.4253 - val_accuracy: 0.8141\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3929 - accuracy: 0.8298 - val_loss: 0.3940 - val_accuracy: 0.8213\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3699 - accuracy: 0.8413 - val_loss: 0.3932 - val_accuracy: 0.8217\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3607 - accuracy: 0.8451 - val_loss: 0.3887 - val_accuracy: 0.8261\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.3534 - accuracy: 0.8491 - val_loss: 0.3899 - val_accuracy: 0.8252\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8521 - val_loss: 0.3931 - val_accuracy: 0.8242\n",
      "1536/1536 - 3s - loss: 0.3972 - accuracy: 0.8227\n",
      "[0.3971611559391022, 0.8226653337478638]\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'sp_ke_park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0029103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"word_vector_dim = 32\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\\nmodel.add(tf.keras.layers.LSTM(16))\\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\nmodel.summary()\\nmodel, history = review_model(train_path, test_path, model, tokenizer = 'hannanum')\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# hannanum은 java.lang.ArrayIndexOutOfBoundsException 예외 발생, 특수문자 때문인 것으로 추정됨.\n",
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'hannanum')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db910099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 너무 오래걸림\\nword_vector_dim = 32\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\\nmodel.add(tf.keras.layers.LSTM(16))\\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\nmodel.summary()\\nmodel, history = review_model(train_path, test_path, model, tokenizer = 'kkma')\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 너무 오래걸림\n",
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'kkma')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5bfa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, None, 32)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 515,281\n",
      "Trainable params: 515,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=spm_input.txt --model_prefix=korean_spm --vocab_size=24000  --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_input.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 24000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: spm_input.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 150000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430599\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1686\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 150000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308597 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 150000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357031\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 357031 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=157511 obj=15.4277 num_tokens=841362 num_tokens/piece=5.34161\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145725 obj=14.3586 num_tokens=846836 num_tokens/piece=5.81119\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109233 obj=14.4548 num_tokens=882029 num_tokens/piece=8.07475\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109051 obj=14.3996 num_tokens=882763 num_tokens/piece=8.09496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81784 obj=14.6341 num_tokens=926725 num_tokens/piece=11.3314\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81776 obj=14.5731 num_tokens=926844 num_tokens/piece=11.3339\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61332 obj=14.8307 num_tokens=968598 num_tokens/piece=15.7927\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61332 obj=14.7702 num_tokens=968692 num_tokens/piece=15.7942\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45999 obj=15.0586 num_tokens=1013498 num_tokens/piece=22.033\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45999 obj=14.9947 num_tokens=1013492 num_tokens/piece=22.0329\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34499 obj=15.3164 num_tokens=1059614 num_tokens/piece=30.7143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34499 obj=15.2502 num_tokens=1059627 num_tokens/piece=30.7147\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26400 obj=15.5785 num_tokens=1103885 num_tokens/piece=41.8138\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26400 obj=15.5122 num_tokens=1103905 num_tokens/piece=41.8146\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n",
      "SentencePiece Tokenizing: 100%|██████████| 146162/146162 [00:05<00:00, 27420.29it/s]\n",
      "SentencePiece Tokenizing: 100%|██████████| 49150/49150 [00:01<00:00, 28895.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 7s 13ms/step - loss: 0.6433 - accuracy: 0.5872 - val_loss: 0.4507 - val_accuracy: 0.8080\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 2s 9ms/step - loss: 0.3699 - accuracy: 0.8434 - val_loss: 0.3513 - val_accuracy: 0.8475\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.3068 - accuracy: 0.8742 - val_loss: 0.3489 - val_accuracy: 0.8461\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2873 - accuracy: 0.8821 - val_loss: 0.3521 - val_accuracy: 0.8461\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.2751 - accuracy: 0.8870 - val_loss: 0.3578 - val_accuracy: 0.8464\n",
      "1536/1536 - 3s - loss: 0.3611 - accuracy: 0.8416\n",
      "[0.36105504631996155, 0.8416480422019958]\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'se', vocab_size=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bed0b674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"word_vector_dim = 32\\nmodel = tf.keras.Sequential()\\nmodel.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\\nmodel.add(tf.keras.layers.LSTM(16))\\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\nmodel.summary()\\nmodel, history = review_model(train_path, test_path, model, tokenizer = 'okt')\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word_vector_dim = 32\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model, history = review_model(train_path, test_path, model, tokenizer = 'okt')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88480115",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015843c",
   "metadata": {},
   "source": [
    "otk, hannanum, kkma를 진행해보고싶었는데, 속도가 느린건지 잘 돌아가지 않아서 시도해보지 못한 게 아쉽다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
